{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64f13d2",
   "metadata": {},
   "source": [
    "# Homework 3 - What is the best anime in the world?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3a15d",
   "metadata": {},
   "source": [
    "- Martina Milazzo\n",
    "- Dimitri Saliola\n",
    "- Roberta Giorgi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b61cb8",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74c803",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 383/383 [05:05<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#383 pages * 50 animes (19130 total)\n",
    "\n",
    "with open(\"C:/Users/marti/Desktop/HW3/list_anime.txt\", \"w\", encoding='utf-8') as file:\n",
    "    \n",
    "    for page in tqdm(range(0, 383)):\n",
    "    \n",
    "        url = 'https://myanimelist.net/topanime.php?limit=' + str(page*50) #single page URL\n",
    "        response = r.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser') #get html\n",
    "\n",
    "        #extract all the links\n",
    "        for tag in soup.find_all(\"tr\", class_=\"ranking-list\"):\n",
    "            a_list = tag.find_all('a', class_=\"hoverinfo_trigger fl-l ml12 mr8\" ,href=True)\n",
    "            for a in a_list:\n",
    "                link = a['href']\n",
    "                file.write(str(link) + '\\n')\n",
    "                \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e0d6f",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EXECUTE THIS CELL#\n",
    "\n",
    "file = open(\"C:/Users/marti/Desktop/HW3/list_anime.txt\", \"r\", encoding='utf-8')\n",
    "\n",
    "pages_dir=\"C:/Users/marti/Desktop/HW3/Pages\"\n",
    "\n",
    "if not os.path.exists(pages_dir):\n",
    "    os.makedirs(pages_dir)\n",
    "\n",
    "counter_anime = 855\n",
    "page = 18\n",
    "\n",
    "for line in file:\n",
    "\n",
    "    counter_anime += 1\n",
    "    if (counter_anime%50 == 1):\n",
    "        page +=1\n",
    "\n",
    "    request = r.get(line, headers={'Cache-Control': 'no-cache'})\n",
    "    #code=request.status_code\n",
    "    if str(request.status_code)[0] == '4': #CODE\n",
    "        while(True): #while(code[0]!='2'):\n",
    "            time.sleep(120)\n",
    "            request = r.get(line, headers={'Cache-Control': 'no-cache'})\n",
    "            print(request.status_code)\n",
    "            if str(request.status_code)[0] == '2': #questa da eliminare\n",
    "                break       #questa pure\n",
    "            \n",
    "\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "    folder_dir=pages_dir+\"/Page\"+str(page)\n",
    "    if not os.path.exists(folder_dir):\n",
    "        os.makedirs(folder_dir)\n",
    "\n",
    "    txt_dir=folder_dir + \"/article_\" + str(counter_anime)+ \".html\"\n",
    "    \n",
    "    with open(txt_dir, \"w\", encoding='utf-8') as file:\n",
    "        file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "729d9b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19131\n"
     ]
    }
   ],
   "source": [
    "# Count anime links in file\n",
    "\n",
    "file = open(\"C:\\\\Users\\\\marti\\\\Desktop\\\\HW3\\\\list_anime.txt\", \"r\", encoding='utf-8')\n",
    "line_count = 0\n",
    "for line in file:\n",
    "    if line != \"\\n\":\n",
    "        line_count += 1\n",
    "file.close()\n",
    "\n",
    "print(line_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0048d",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac451974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EXECUTE THIS CELL#\n",
    "\n",
    "title = []\n",
    "types = []\n",
    "numEpisode = []\n",
    "release = []\n",
    "end = []\n",
    "numMembers = []\n",
    "score = []\n",
    "users = []\n",
    "rank = []\n",
    "popularity = []\n",
    "description = []\n",
    "related = []\n",
    "characters = []\n",
    "voices = []\n",
    "staff = []\n",
    "\n",
    "pages = os.listdir(\"C:/Users/marti/Desktop/HW3/Pages\")[1:]\n",
    "\n",
    "for p in pages:\n",
    "    n_html = os.listdir(pages+\"/\"+str(p))\n",
    "    for i in range(1, len(n_html)+1):\n",
    "        file = open(pages+\"/\"+str(p)+\"/article_\"+str(i), 'r', encoding=\"utf8\")\n",
    "        anime = BeautifulSoup(file, 'lxml')\n",
    "        \n",
    "        #TITLE\n",
    "        try:\n",
    "            t=anime.strong.contents[0]\n",
    "            title.append(t)\n",
    "        except:\n",
    "            title.append('NA')\n",
    "            \n",
    "\n",
    "        #TYPE\n",
    "        try:\n",
    "            ty=anime.find(text = 'Type:').find_next('a').contents[0]\n",
    "            types.append(ty)\n",
    "        except:\n",
    "            types.append('NA')\n",
    "\n",
    "\n",
    "        #NUM_EPISODE\n",
    "        try:\n",
    "            n=int(anime.find(text = 'Episodes:').next_element.strip())\n",
    "            numEpisode.append(n)\n",
    "        except:\n",
    "            numEpisode.append(n)\n",
    "\n",
    "\n",
    "        #RELEASE_DATE\n",
    "        try:\n",
    "            date = anime.find(text = 'Aired:').next_element.strip()\n",
    "            if len(date)>15:\n",
    "                rl=datetime.strptime(date.split(\" to \")[0], '%b %d, %Y' )\n",
    "            else:\n",
    "                rl=datetime.strptime(date, '%b %d, %Y' )\n",
    "            release.append(rl)\n",
    "        except:\n",
    "            release.append('NA')\n",
    "\n",
    "\n",
    "        #END_DATE\n",
    "        try:\n",
    "            date = anime.find(text = 'Aired:').next_element.strip()\n",
    "            if len(date)>15 and date.split(\" to \")[1] != \"?\":\n",
    "                e=datetime.strptime(date.split(\" to \")[1], '%b %d, %Y' )\n",
    "            else:\n",
    "                e=pd.to_datetime(np.NaN, errors='coerce')\n",
    "            end.append(e)\n",
    "        except:\n",
    "            end.append('NA')\n",
    "\n",
    "\n",
    "        #NUM_MEMBER\n",
    "        try:\n",
    "            num_mem = anime.find(text = 'Members:').next_element\n",
    "            nm=int(num_mem.replace('n', '').replace(',', '').strip())\n",
    "            numMembers.append(nm)\n",
    "        except:\n",
    "            numMembers.append(0)\n",
    "\n",
    "\n",
    "        #SCORE\n",
    "        try:\n",
    "            s=anime.find(text = 'Score:').find_next('span').contents\n",
    "            s=float(s[0])\n",
    "            score.append(s)\n",
    "        except:\n",
    "            score.append(None)\n",
    "\n",
    "\n",
    "        #USERS\n",
    "        try:\n",
    "            us = anime.find(text = 'Score:').find_next('span').find_next('span').contents\n",
    "            u=int(us[0])\n",
    "            users.append(u)\n",
    "        except:\n",
    "            users.append(0)\n",
    "\n",
    "\n",
    "        #RANK\n",
    "        try:\n",
    "            rk = anime.find(text = 'Ranked:').next_element\n",
    "            rk=int(rk.replace('\\n', '').replace('#', '').strip())\n",
    "            rank.append(rk)\n",
    "        except:\n",
    "            rank.append(None)\n",
    "\n",
    "\n",
    "        #POPULARITY\n",
    "        try:\n",
    "            pop = anime.find(text='Popularity:').next_element\n",
    "            pop=int(pop.replace(\"\\n\",\"\").replace('#', '').strip())\n",
    "            popularity.append(pop)\n",
    "        except:\n",
    "            popularity.append(None)\n",
    "\n",
    "\n",
    "        #DESCRIPTION\n",
    "        try:\n",
    "            des=anime.find(text = 'Synopsis:').find_next('p').text\n",
    "            des=des.replace(\"\\n\",\"\")\n",
    "            description.append(des)\n",
    "        except:\n",
    "            description.append('NA')\n",
    "\n",
    "\n",
    "        #RELATED\n",
    "        list = anime.find(text = 'Related Anime')\n",
    "        rel = []\n",
    "\n",
    "        if(list != None):\n",
    "\n",
    "            try:    \n",
    "                list = list.find_next('list')\n",
    "                list = list.find_all('a')\n",
    "\n",
    "                for t in list:\n",
    "                    rel.append(t.text)\n",
    "\n",
    "            except:\n",
    "                for t in list:\n",
    "                    rel.append('NA')\n",
    "\n",
    "        related.append(rel)\n",
    "\n",
    "        #CHARACTER\n",
    "        c = []\n",
    "        try:\n",
    "            list = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "            list = list.find_all('list')\n",
    "\n",
    "            for t in list:\n",
    "                people = t.find_all('h3')\n",
    "                for p in people:\n",
    "                    c.append(p.text)\n",
    "        except:\n",
    "            c.append('NA')\n",
    "            \n",
    "        characters.append(c)\n",
    "\n",
    "\n",
    "        #VOICES\n",
    "        v=[]\n",
    "        try:\n",
    "            list = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "            list = list.find_all('h3')\n",
    "\n",
    "            for t in list:\n",
    "                people = el.find_next('list')\n",
    "                for person in people:\n",
    "                    v.append(person.find('a').text)\n",
    "        except:\n",
    "            v.append('NA')\n",
    "\n",
    "        voices.append(v)\n",
    "\n",
    "        #STAFF\n",
    "        st = []\n",
    "        try:\n",
    "            list = anime.find_all(text = 'Staff')[1].find_next(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "            \n",
    "            if(list != None):    \n",
    "                table = list.find_all(\"table\")\n",
    "                for t in table:\n",
    "                    i = t.find_all(\"td\")[1]\n",
    "                    p = [i.find(\"a\").text, i.find(\"small\").text]\n",
    "                    st.append(p)\n",
    "        except:\n",
    "            st.append(p)\n",
    "\n",
    "        staff.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b45399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EXECUTE THIS CELL#\n",
    "\n",
    "col = ['animeTitle', 'animeType', 'animeNumEpisode','releaseDate', 'endDate', 'animeNumMembers', 'animeScore', 'animeUsers', \n",
    "       'animeRank', 'animePopularity', 'animdeDescription', 'animeRelated', 'animeCharacters', 'animeVoices', 'animeStaff']\n",
    "\n",
    "types = {'animeTitle' : 'object', \n",
    "         'animeType' : 'object', \n",
    "         'animeNumEpisode' : 'int64',\n",
    "         'releaseDate' : 'datetime64', \n",
    "         'endDate' : 'datetime64', \n",
    "         'animeNumMembers' : 'int64', \n",
    "         'animeScore' : 'float64',\n",
    "         'animeUsers' : 'int64', \n",
    "         'animeRank' : 'int64',\n",
    "         'animePopularity' : 'int64',\n",
    "         'animdeDescription' : 'object',\n",
    "         'animeRelated' : 'object',\n",
    "         'animeCharacters' : 'object',\n",
    "         'animeVoices' : 'object',\n",
    "         'animeStaff' : 'object'}\n",
    "\n",
    "data = list(zip(title,types,numEpisode,release,end,numMembers,score,users,rank,popularity,description,related,characters,voices,staff))\n",
    "\n",
    "dataset = pd.DataFrame(data, columns = col).astype(dtype = types)  \n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    with open('C:/Users/marti/Desktop/HW3/tsv/anime_'+str(i)+'.tsv', 'w') as file:\n",
    "        tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "        tsv_writer.writerow([x for x in dataset.columns]) \n",
    "        tsv_writer.writerow(x for x in dataset.iloc[i]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac4605-7169-4017-9fca-a4be171fee2d",
   "metadata": {},
   "source": [
    "# 2. Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae83e1c0-baca-4144-a926-8d996e42ca59",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/marti/Desktop/tsv_files_full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c23802dca578>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Get a list of the animes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0manimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:/Users/marti/Desktop/tsv_files_full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Save column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/marti/Desktop/tsv_files_full'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "\n",
    "# Get a list of the animes\n",
    "animes = os.listdir('C:/Users/marti/Desktop/tsv_files_full')\n",
    "\n",
    "# Save column names\n",
    "names = ['Title', 'Type', 'Episodes', 'Members', 'Score', 'Users', 'Rank', 'Popularity', 'Description', 'Related', 'Characters', 'Voices', 'Staff', 'release', 'end']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1f769-c58b-4c10-9dc5-c91ea53e380c",
   "metadata": {},
   "source": [
    "### 2.0.1 Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e802475-ee90-4ec9-999c-5554c1cf5876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "298db704-bd30-4f01-b69f-551a62a12bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(anime):\n",
    "    \n",
    "    # Save the English stopwords in a variable\n",
    "    en_stops = set(stopwords.words('english'))\n",
    "    \n",
    "    for column in anime:\n",
    "        processed_prop = ''\n",
    "        for prop in anime[column]:\n",
    "            if type(prop) is str:\n",
    "                for word in prop.split():\n",
    "                    if word not in en_stops:\n",
    "                        processed_prop += word + ' '\n",
    "                processed_df.at[0, column] = processed_prop\n",
    "            else:\n",
    "                processed_df.at[0, column] = prop\n",
    "                \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d10f13-7ada-44c0-b385-d5ef8802e36f",
   "metadata": {},
   "source": [
    "### 2.0.2 Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e9ff906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c662063-b5c1-4022-a61f-424f4813e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(anime):\n",
    "    \n",
    "    for column in anime:\n",
    "        for prop in anime[column]:\n",
    "            words = nltk.word_tokenize(str(prop))\n",
    "            processed = [word for word in words if word.isalnum()]\n",
    "            processed_df.at[0, column] = ' '.join(processed)\n",
    "            \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e765e8-65af-480c-800d-0452243204f4",
   "metadata": {},
   "source": [
    "### 2.0.3 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ddfb9b7-501b-499e-940e-23f520a41df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(anime):\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for column in anime:\n",
    "        processed_prop = ''\n",
    "        for prop in anime[column]:\n",
    "            if type(prop) is str:\n",
    "                for word in prop.split():\n",
    "                    processed_prop += ps.stem(word) + ' '\n",
    "                processed_df.at[0, column] = processed_prop\n",
    "            else:\n",
    "                processed_df.at[0, column] = prop\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e47e0872-f8ba-4945-a199-bf2efc3b0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anime_i in range(len(animes)):\n",
    "    \n",
    "    anime = pd.read_csv('C:/Users/marti/Desktop/tsv_files_full/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "\n",
    "    processed_df = pd.DataFrame(columns=names)    \n",
    "    processed_df = removeStopwords(anime)\n",
    "    processed_df = removePunctuation(processed_df)\n",
    "    processed_df = stemming(processed_df)\n",
    "\n",
    "    processed_df.to_csv('C:/Users/marti/Desktop/tsv_processed/anime_{i}.tsv'.format(i=anime_i), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980fa2b-3543-49aa-a1c1-5ed66bef7b5e",
   "metadata": {},
   "source": [
    "### 2.1.1 Create the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c1eab05-9cf5-489a-af81-2223f41bb37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_lst = []\n",
    "for anime_i in range(len(animes)):\n",
    "    anime = pd.read_csv('C:/Users/marti/Desktop/tsv_processed/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "    animes_lst.append(anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcab93ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Title Type  Episodes  Members Score    Users  \\\n",
      "0    fullmet alchemist brotherhood   tv         64  2675751   NaN  1622384   \n",
      "1                               NaN  tv         51   483807   NaN   169476   \n",
      "2  shingeki kyojin season 3 part 2   tv         10  1596039   NaN  1087519   \n",
      "3                       stein gate   tv         24  2090910   NaN  1109700   \n",
      "4           fruit basket the final   tv         13   275214   NaN   113310   \n",
      "\n",
      "  Rank  Popularity                                        Description  \\\n",
      "0    1           3  after horrif alchemi experi goe wrong elric ho...   \n",
      "1    2         337  gintoki shinpachi kagura return broke member y...   \n",
      "2    3          33  restor diminish hope survey corp embark missio...   \n",
      "3    4          11  mad scientist rintar okab rent room ricketi ol...   \n",
      "4    5         651  year ago chines zodiac spirit god swore stay t...   \n",
      "\n",
      "                                             Related  \\\n",
      "0  fullmet alchemist version fullmet alchemist st...   \n",
      "1  gintama gintama movi 2 yorozuya yo eien nare s...   \n",
      "2  shingeki kyojin shingeki kyojin season 3 shing...   \n",
      "3  stein gate set chäo head stein gate oukoubakko...   \n",
      "4  fruit basket fruit basket 2nd season fruit bas...   \n",
      "\n",
      "                                          Characters  \\\n",
      "0  edward alphons roy mae riza ling alex loui win...   \n",
      "1  gintoki shinpachi kotar toushir sougo shinsuk ...   \n",
      "2  eren mikasa armin erwin hang sasha reiner jean...   \n",
      "3  rintar kurisu mayuri itaru suzuha ruka rumiho ...   \n",
      "4  kyou tooru yuki hatsuharu momiji shigur ayam h...   \n",
      "\n",
      "                                              Voices  \\\n",
      "0  romi rie shinichiro keiji yuuichi fumiko mamor...   \n",
      "1  tomokazu rie daisuk akira kazuya kenichi takeh...   \n",
      "2  hiroshi yuki yui marina daisuk romi yuu yoshim...   \n",
      "3  mamoru asami kana tomokazu yukari yuu haruko s...   \n",
      "4  yuuma manaka nobunaga makoto megumi yuuichi ta...   \n",
      "\n",
      "                                               Staff release   end  \n",
      "0  justin noritomo yasuhiro episod director story...    nan   nan   \n",
      "1  youichi storyboard plan chizuru storyboard key...    nan   nan   \n",
      "2                shuuhei jouji katsuji plan tetsuya     nan   nan   \n",
      "3  gaku takeshi plan hiroshi episod director stor...    nan   nan   \n",
      "4  yoshihid jin director up song perform song per...    nan   nan   \n"
     ]
    }
   ],
   "source": [
    "animes_df = pd.concat(animes_lst)\n",
    "animes_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "animes_df.reset_index(inplace = True, drop=True)\n",
    "print(animes_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e5db8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(dataframe):\n",
    "    vocabulary = {}\n",
    "    term_id = 1\n",
    "\n",
    "    for index, anime in dataframe.iterrows():\n",
    "\n",
    "        description = anime['Description']  # 'Ciao mi chiamo Roberta'\n",
    "        if type(description) is str:\n",
    "            words = description.split()  # ['ciao', 'mi', 'chiamo', 'Roberta']\n",
    "\n",
    "            for word in words:\n",
    "\n",
    "                # Create vocabulary\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = term_id\n",
    "                    term_id += 1\n",
    "          \n",
    "    return vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83d9a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(vocabulary, dataframe):\n",
    "    inverted_index={}\n",
    "    for index, anime in dataframe.iterrows():\n",
    "\n",
    "        description = anime['Description']  # 'Ciao mi chiamo Roberta'\n",
    "        if type(description) is str:\n",
    "            words = description.split()  # ['ciao', 'mi', 'chiamo', 'Roberta']\n",
    "\n",
    "            for word in words:\n",
    "                term_id = vocabulary[word]\n",
    "                if term_id not in inverted_index:\n",
    "                    inverted_index[term_id] = []\n",
    "                if index not in inverted_index[term_id]:\n",
    "                    inverted_index[term_id].append(index)\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35905078",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=create_vocabulary(animes_df)\n",
    "inverted_index=create_inverted_index(vocabulary, animes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f2db0",
   "metadata": {},
   "source": [
    "## 2.1.2 execute query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bb53306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary to return the original description of the anime (not the processed one)\n",
    "animes_original = []\n",
    "for anime_i in range(len(animes)):\n",
    "    anime_original = pd.read_csv('C:/Users/marti/Desktop/tsv_files_full/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "    animes_original.append(anime_original)\n",
    "animes_original = pd.concat(animes_original)\n",
    "animes_original = animes_original.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85d92a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "animes_original.to_csv('C:/Users/marti/Desktop/file_originali_tsv/anime_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60405d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary to get the urls for each anime\n",
    "with open('C:/Users/marti/Desktop/HW3/list_anime.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e398f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query):\n",
    "    #query=\"after\"\n",
    "    term_id=0\n",
    "    first=True\n",
    "    query=query.split()\n",
    "    q_match=pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "\n",
    "    try:\n",
    "        for term in query:\n",
    "            if term in vocabulary:\n",
    "                term_id=vocabulary[term]\n",
    "            if first:\n",
    "                lst_doc=inverted_index[term_id]\n",
    "                first=False\n",
    "            else:\n",
    "                lst_doc=set(lst_doc).intersection(inverted_index[term_id])\n",
    "                if len(lst_doc)==0:\n",
    "                    print('no doc found')\n",
    "                    break\n",
    "\n",
    "        for i, doc in enumerate(lst_doc):\n",
    "            q_match.at[i, ['Title', 'Description', 'URL']]=animes_original.iloc[doc]['Title'], animes_original.iloc[doc]['Description'], lines[doc]\n",
    "            #q_match.at[i, 'Description']=animes_original.iloc[doc]['Description']\n",
    "            #q_match.at[i, 'URL']=lines[doc]\n",
    "        \n",
    "        display(q_match)\n",
    "    except:\n",
    "        print(\"no doc found\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f29dea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no doc found\n"
     ]
    }
   ],
   "source": [
    "query=\"saiyan race\"\n",
    "tokenized_query = nltk.word_tokenize(query)\n",
    "\n",
    "# Process the query before executing it\n",
    "processed_query = \"\"\n",
    "for word in tokenized_query:\n",
    "    \n",
    "    # Remove punctuation\n",
    "    alnum_query_lst = [word for word in words if word.isalnum()]\n",
    "    processed_query += ' '.join(alnum_query_lst)\n",
    "    \n",
    "    for word in processed_query:\n",
    "        # Remove stopwords\n",
    "        if word not in en_stops:\n",
    "            processed_prop -= word\n",
    "    \n",
    "    processed_query += ps.stem(word) + ' '\n",
    "    \n",
    "processed = [word for word in words if word.isalnum()]\n",
    "processed_df.at[0, column] = ' '.join(processed)\n",
    "\n",
    "# Execute the processed query\n",
    "execute_query(processed_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf60158",
   "metadata": {},
   "source": [
    "## 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f58ae",
   "metadata": {},
   "source": [
    "### 2.2.1) inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b590932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_second_inverted_index(vocabulary, inverted_index, dataframe):\n",
    "\n",
    "    second_inverted_index={}\n",
    "    \n",
    "    for index, anime in dataframe.iterrows():\n",
    "        vocabulary_occurrences = {}\n",
    "        occurrence=0\n",
    "\n",
    "        description = anime['Description']  # 'Ciao mi chiamo Roberta'\n",
    "        if type(description) is str:\n",
    "            words = description.split()  # ['ciao', 'mi', 'chiamo', 'Roberta']\n",
    "\n",
    "            for word in words:\n",
    "                occurrence+=1\n",
    "                vocabulary_occurrences[word]=occurrence\n",
    "\n",
    "            tot_words_in_doc=len(vocabulary_occurrences)\n",
    "\n",
    "            for word in vocabulary_occurrences:\n",
    "                term_id=vocabulary[word]\n",
    "\n",
    "                #term frequency=#word appears in doc / #tot words in doc\n",
    "                word_occur=vocabulary_occurrences[word]\n",
    "                tf=word_occur/tot_words_in_doc\n",
    "\n",
    "                #inverse data frequency= log(#tot_docs/#docs_containing_term_i)\n",
    "                tot_docs=len(dataframe)\n",
    "                docs_with_word=len(inverted_index[term_id])\n",
    "                idf=math.log(tot_docs/docs_with_word, 10)\n",
    "\n",
    "                #tf-idf\n",
    "                tf_idf=tf*idf\n",
    "\n",
    "                if term_id not in second_inverted_index:\n",
    "                    second_inverted_index[term_id] = []\n",
    "                if index not in second_inverted_index[term_id]:\n",
    "                    second_inverted_index[term_id].append((index, tf_idf))\n",
    "\n",
    "    return second_inverted_index\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95a7f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#animes_df = pd.read_csv('/path') #da ricontrollare\n",
    "vocabulary = create_vocabulary(animes_df)\n",
    "inverted_index = create_inverted_index(vocabulary, animes_df)\n",
    "\n",
    "second_inverted_index=create_second_inverted_index(vocabulary, inverted_index, animes_df)\n",
    "\n",
    "#print(second_inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8302be",
   "metadata": {},
   "source": [
    "### 2.2.2) Execute query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query):\n",
    "\n",
    "    cos_similarities = []\n",
    "    # Aggiungere URL\n",
    "    q_match=pd.DataFrame(columns=['Title', 'Description', 'Similarity'])\n",
    "\n",
    "    lst_doc = []\n",
    "    for term in query.split():\n",
    "        if term in vocabulary:\n",
    "            term_id = vocabulary[term]\n",
    "            lst_doc.append(set(inverted_index[term_id]))\n",
    "\n",
    "    lst_intersect = set.intersection(*lst_doc)\n",
    "\n",
    "    for term in query.split():\n",
    "        word_occurrence_query = query.count(term)\n",
    "        c = 0\n",
    "        norm1 = 0\n",
    "        norm2 = 0\n",
    "\n",
    "        for i in lst_doc:\n",
    "            c += second_inverted_index[i][0][1] * word_occurrence_query\n",
    "\n",
    "        for term_id in second_inverted_index:\n",
    "            i = second_inverted_index[term_id][0][1]\n",
    "            norm1 += np.power(i, 2)\n",
    "        norm1 = np.sqrt(norm1)\n",
    "\n",
    "        norm2 += np.power(word_occurrence_query, 2)\n",
    "        norm2 = np.sqrt(norm2)\n",
    "\n",
    "        cos_similarities.append(c / (norm1 * norm2))\n",
    "\n",
    "\n",
    "    for i, doc in enumerate(lst_intersect):\n",
    "        # Aggiungere URL\n",
    "        q_match.at[i, ['Title', 'Description', 'Similarity']] = animes_original.iloc[doc]['Title'], \\\n",
    "                                                                animes_original.iloc[doc]['Description'], cos_similarities[\n",
    "                                                                    i]\n",
    "    display(q_match)\n",
    "    return q_match\n",
    "\n",
    "\n",
    "print(cosine_similarity_dict('after'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7aa32b",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cabccc",
   "metadata": {},
   "source": [
    "### aggiungere pseudo codice\n",
    "In the given problem, the goal is to choose the maximum number of appointments such as the appointments choosen are not consecutive. It means that the personal trainer want needs a break between appointments and so he can't accept two consecutive request. Obviously, all the request are consider in chronological order. The input data is just a list of the requested appointment, and we want to maximize the sum of value (not adiacent) contained in the list.\n",
    "\n",
    "One solution could be the followed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sum(app):\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    tmp1 = 0\n",
    "    tmp2 = 0\n",
    "    for i in range(len(app)):\n",
    "        if ((i % 2) == 0):\n",
    "            tmp1 += app[i]\n",
    "            l1.append(app[i])\n",
    "        else:\n",
    "            tmp2 += app[i]\n",
    "            l2.append(app[i])\n",
    "    if (tmp1 > tmp2):\n",
    "        return l1, tmp1\n",
    "    else:\n",
    "        return l2, tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appointments required: [30, 40, 25, 50, 30, 20] \n",
      "\n",
      "List of appointment accepted:  [40, 50, 20]\n",
      "total duration (of the accepted appointment):  110\n"
     ]
    }
   ],
   "source": [
    "app = [30, 40, 25, 50, 30, 20] \n",
    "print('Appointments required:', app, '\\n') \n",
    "\n",
    "lst, duration=max_sum(app)\n",
    "print('List of appointment accepted: ', lst)\n",
    "print('total duration (of the accepted appointment): ', duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dcfc74",
   "metadata": {},
   "source": [
    "In this program we observe each element of the list twice (one from the even index, and one from the odd index), both of this going through the list with a step of 2. Thenwe add the odd and even values ​​respectively, and then compare them. At the end, we choose the path that maximize the sum."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
