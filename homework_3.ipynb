{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64f13d2",
   "metadata": {},
   "source": [
    "# Homework 3 - What is the best anime in the world?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3a15d",
   "metadata": {},
   "source": [
    "- Martina Milazzo\n",
    "- Dimitri Saliola\n",
    "- Roberta Giorgi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b61cb8",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74c803",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 383/383 [05:05<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#383 pages * 50 animes (19130 total)\n",
    "\n",
    "with open(\"C:/Users/marti/Desktop/HW3/list_anime.txt\", \"w\", encoding='utf-8') as file:\n",
    "    \n",
    "    for page in tqdm(range(0, 383)):\n",
    "    \n",
    "        url = 'https://myanimelist.net/topanime.php?limit=' + str(page*50) #single page URL\n",
    "        response = r.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser') #get html\n",
    "\n",
    "        #extract all the links\n",
    "        for tag in soup.find_all(\"tr\", class_=\"ranking-list\"):\n",
    "            a_list = tag.find_all('a', class_=\"hoverinfo_trigger fl-l ml12 mr8\" ,href=True)\n",
    "            for a in a_list:\n",
    "                link = a['href']\n",
    "                file.write(str(link) + '\\n')\n",
    "                \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e0d6f",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EXECUTE THIS CELL#\n",
    "\n",
    "file = open(\"C:/Users/marti/Desktop/HW3/list_anime.txt\", \"r\", encoding='utf-8')\n",
    "\n",
    "pages_dir=\"C:/Users/marti/Desktop/HW3/Pages\"\n",
    "\n",
    "if not os.path.exists(pages_dir):\n",
    "    os.makedirs(pages_dir)\n",
    "\n",
    "counter_anime = 855\n",
    "page = 18\n",
    "\n",
    "for line in file:\n",
    "\n",
    "    counter_anime += 1\n",
    "    if (counter_anime%50 == 1):\n",
    "        page +=1\n",
    "\n",
    "    request = r.get(line, headers={'Cache-Control': 'no-cache'})\n",
    "    #code=request.status_code\n",
    "    if str(request.status_code)[0] == '4': #CODE\n",
    "        while(True): #while(code[0]!='2'):\n",
    "            time.sleep(120)\n",
    "            request = r.get(line, headers={'Cache-Control': 'no-cache'})\n",
    "            print(request.status_code)\n",
    "            if str(request.status_code)[0] == '2': #questa da eliminare\n",
    "                break       #questa pure\n",
    "            \n",
    "\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "    folder_dir=pages_dir+\"/Page\"+str(page)\n",
    "    if not os.path.exists(folder_dir):\n",
    "        os.makedirs(folder_dir)\n",
    "\n",
    "    txt_dir=folder_dir + \"/article_\" + str(counter_anime)+ \".html\"\n",
    "    \n",
    "    with open(txt_dir, \"w\", encoding='utf-8') as file:\n",
    "        file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "729d9b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19131\n"
     ]
    }
   ],
   "source": [
    "# Count anime links in file\n",
    "\n",
    "file = open(\"C:\\\\Users\\\\marti\\\\Desktop\\\\HW3\\\\list_anime.txt\", \"r\", encoding='utf-8')\n",
    "line_count = 0\n",
    "for line in file:\n",
    "    if line != \"\\n\":\n",
    "        line_count += 1\n",
    "file.close()\n",
    "\n",
    "print(line_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0048d",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac451974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EXECUTE THIS CELL#\n",
    "\n",
    "title = []\n",
    "types = []\n",
    "numEpisode = []\n",
    "release = []\n",
    "end = []\n",
    "numMembers = []\n",
    "score = []\n",
    "users = []\n",
    "rank = []\n",
    "popularity = []\n",
    "description = []\n",
    "related = []\n",
    "characters = []\n",
    "voices = []\n",
    "staff = []\n",
    "\n",
    "pages = os.listdir(\"C:/Users/marti/Desktop/HW3/Pages\")[1:]\n",
    "\n",
    "for p in pages:\n",
    "    n_html = os.listdir(pages+\"/\"+str(p))\n",
    "    for i in range(1, len(n_html)+1):\n",
    "        file = open(pages+\"/\"+str(p)+\"/article_\"+str(i), 'r', encoding=\"utf8\")\n",
    "        anime = BeautifulSoup(file, 'lxml')\n",
    "        \n",
    "        #TITLE\n",
    "        try:\n",
    "            t=anime.strong.contents[0]\n",
    "            title.append(t)\n",
    "        except:\n",
    "            title.append('NA')\n",
    "            \n",
    "\n",
    "        #TYPE\n",
    "        try:\n",
    "            ty=anime.find(text = 'Type:').find_next('a').contents[0]\n",
    "            types.append(ty)\n",
    "        except:\n",
    "            types.append('NA')\n",
    "\n",
    "\n",
    "        #NUM_EPISODE\n",
    "        try:\n",
    "            n=int(anime.find(text = 'Episodes:').next_element.strip())\n",
    "            numEpisode.append(n)\n",
    "        except:\n",
    "            numEpisode.append(n)\n",
    "\n",
    "\n",
    "        #RELEASE_DATE\n",
    "        try:\n",
    "            date = anime.find(text = 'Aired:').next_element.strip()\n",
    "            if len(date)>15:\n",
    "                rl=datetime.strptime(date.split(\" to \")[0], '%b %d, %Y' )\n",
    "            else:\n",
    "                rl=datetime.strptime(date, '%b %d, %Y' )\n",
    "            release.append(rl)\n",
    "        except:\n",
    "            release.append('NA')\n",
    "\n",
    "\n",
    "        #END_DATE\n",
    "        try:\n",
    "            date = anime.find(text = 'Aired:').next_element.strip()\n",
    "            if len(date)>15 and date.split(\" to \")[1] != \"?\":\n",
    "                e=datetime.strptime(date.split(\" to \")[1], '%b %d, %Y' )\n",
    "            else:\n",
    "                e=pd.to_datetime(np.NaN, errors='coerce')\n",
    "            end.append(e)\n",
    "        except:\n",
    "            end.append('NA')\n",
    "\n",
    "\n",
    "        #NUM_MEMBER\n",
    "        try:\n",
    "            num_mem = anime.find(text = 'Members:').next_element\n",
    "            nm=int(num_mem.replace('n', '').replace(',', '').strip())\n",
    "            numMembers.append(nm)\n",
    "        except:\n",
    "            numMembers.append(0)\n",
    "\n",
    "\n",
    "        #SCORE\n",
    "        try:\n",
    "            s=anime.find(text = 'Score:').find_next('span').contents\n",
    "            s=float(s[0])\n",
    "            score.append(s)\n",
    "        except:\n",
    "            score.append(None)\n",
    "\n",
    "\n",
    "        #USERS\n",
    "        try:\n",
    "            us = anime.find(text = 'Score:').find_next('span').find_next('span').contents\n",
    "            u=int(us[0])\n",
    "            users.append(u)\n",
    "        except:\n",
    "            users.append(0)\n",
    "\n",
    "\n",
    "        #RANK\n",
    "        try:\n",
    "            rk = anime.find(text = 'Ranked:').next_element\n",
    "            rk=int(rk.replace('\\n', '').replace('#', '').strip())\n",
    "            rank.append(rk)\n",
    "        except:\n",
    "            rank.append(None)\n",
    "\n",
    "\n",
    "        #POPULARITY\n",
    "        try:\n",
    "            pop = anime.find(text='Popularity:').next_element\n",
    "            pop=int(pop.replace(\"\\n\",\"\").replace('#', '').strip())\n",
    "            popularity.append(pop)\n",
    "        except:\n",
    "            popularity.append(None)\n",
    "\n",
    "\n",
    "        #DESCRIPTION\n",
    "        try:\n",
    "            des=anime.find(text = 'Synopsis:').find_next('p').text\n",
    "            des=des.replace(\"\\n\",\"\")\n",
    "            description.append(des)\n",
    "        except:\n",
    "            description.append('NA')\n",
    "\n",
    "\n",
    "        #RELATED\n",
    "        list = anime.find(text = 'Related Anime')\n",
    "        rel = []\n",
    "\n",
    "        if(list != None):\n",
    "\n",
    "            try:    \n",
    "                list = list.find_next('list')\n",
    "                list = list.find_all('a')\n",
    "\n",
    "                for t in list:\n",
    "                    rel.append(t.text)\n",
    "\n",
    "            except:\n",
    "                for t in list:\n",
    "                    rel.append('NA')\n",
    "\n",
    "        related.append(rel)\n",
    "\n",
    "        #CHARACTER\n",
    "        c = []\n",
    "        try:\n",
    "            list = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "            list = list.find_all('list')\n",
    "\n",
    "            for t in list:\n",
    "                people = t.find_all('h3')\n",
    "                for p in people:\n",
    "                    c.append(p.text)\n",
    "        except:\n",
    "            c.append('NA')\n",
    "            \n",
    "        characters.append(c)\n",
    "\n",
    "\n",
    "        #VOICES\n",
    "        v=[]\n",
    "        try:\n",
    "            list = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "            list = list.find_all('h3')\n",
    "\n",
    "            for t in list:\n",
    "                people = el.find_next('list')\n",
    "                for person in people:\n",
    "                    v.append(person.find('a').text)\n",
    "        except:\n",
    "            v.append('NA')\n",
    "\n",
    "        voices.append(v)\n",
    "\n",
    "        #STAFF\n",
    "        st = []\n",
    "        try:\n",
    "            list = anime.find_all(text = 'Staff')[1].find_next(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "            \n",
    "            if(list != None):    \n",
    "                table = list.find_all(\"table\")\n",
    "                for t in table:\n",
    "                    i = t.find_all(\"td\")[1]\n",
    "                    p = [i.find(\"a\").text, i.find(\"small\").text]\n",
    "                    st.append(p)\n",
    "        except:\n",
    "            st.append(p)\n",
    "\n",
    "        staff.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b45399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EXECUTE THIS CELL#\n",
    "\n",
    "col = ['animeTitle', 'animeType', 'animeNumEpisode','releaseDate', 'endDate', 'animeNumMembers', 'animeScore', 'animeUsers', \n",
    "       'animeRank', 'animePopularity', 'animdeDescription', 'animeRelated', 'animeCharacters', 'animeVoices', 'animeStaff']\n",
    "\n",
    "types = {'animeTitle' : 'object', \n",
    "         'animeType' : 'object', \n",
    "         'animeNumEpisode' : 'int64',\n",
    "         'releaseDate' : 'datetime64', \n",
    "         'endDate' : 'datetime64', \n",
    "         'animeNumMembers' : 'int64', \n",
    "         'animeScore' : 'float64',\n",
    "         'animeUsers' : 'int64', \n",
    "         'animeRank' : 'int64',\n",
    "         'animePopularity' : 'int64',\n",
    "         'animdeDescription' : 'object',\n",
    "         'animeRelated' : 'object',\n",
    "         'animeCharacters' : 'object',\n",
    "         'animeVoices' : 'object',\n",
    "         'animeStaff' : 'object'}\n",
    "\n",
    "data = list(zip(title,types,numEpisode,release,end,numMembers,score,users,rank,popularity,description,related,characters,voices,staff))\n",
    "\n",
    "dataset = pd.DataFrame(data, columns = col).astype(dtype = types)  \n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    with open('C:/Users/marti/Desktop/HW3/tsv/anime_'+str(i)+'.tsv', 'w') as file:\n",
    "        tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "        tsv_writer.writerow([x for x in dataset.columns]) \n",
    "        tsv_writer.writerow(x for x in dataset.iloc[i]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac4605-7169-4017-9fca-a4be171fee2d",
   "metadata": {},
   "source": [
    "# 2. Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae83e1c0-baca-4144-a926-8d996e42ca59",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/marti/Desktop/tsv_files_full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c23802dca578>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Get a list of the animes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0manimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:/Users/marti/Desktop/tsv_files_full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Save column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/marti/Desktop/tsv_files_full'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "\n",
    "# Get a list of the animes\n",
    "animes = os.listdir('C:/Users/marti/Desktop/tsv_files_full')\n",
    "\n",
    "# Save column names\n",
    "names = ['Title', 'Type', 'Episodes', 'Members', 'Score', 'Users', 'Rank', 'Popularity', 'Description', 'Related', 'Characters', 'Voices', 'Staff', 'release', 'end']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1f769-c58b-4c10-9dc5-c91ea53e380c",
   "metadata": {},
   "source": [
    "### 2.0.1 Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e802475-ee90-4ec9-999c-5554c1cf5876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "298db704-bd30-4f01-b69f-551a62a12bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(anime):\n",
    "    \n",
    "    # Save the English stopwords in a variable\n",
    "    en_stops = set(stopwords.words('english'))\n",
    "    \n",
    "    for column in anime:\n",
    "        processed_prop = ''\n",
    "        for prop in anime[column]:\n",
    "            if type(prop) is str:\n",
    "                for word in prop.split():\n",
    "                    if word not in en_stops:\n",
    "                        processed_prop += word + ' '\n",
    "                processed_df.at[0, column] = processed_prop\n",
    "            else:\n",
    "                processed_df.at[0, column] = prop\n",
    "                \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d10f13-7ada-44c0-b385-d5ef8802e36f",
   "metadata": {},
   "source": [
    "### 2.0.2 Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3aea3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c662063-b5c1-4022-a61f-424f4813e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(anime):\n",
    "    \n",
    "    for column in anime:\n",
    "        for prop in anime[column]:\n",
    "            words = nltk.word_tokenize(str(prop))\n",
    "            processed = [word for word in words if word.isalnum()]\n",
    "            processed_df.at[0, column] = ' '.join(processed)\n",
    "            \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e765e8-65af-480c-800d-0452243204f4",
   "metadata": {},
   "source": [
    "### 2.0.3 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ddfb9b7-501b-499e-940e-23f520a41df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(anime):\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for column in anime:\n",
    "        processed_prop = ''\n",
    "        for prop in anime[column]:\n",
    "            if type(prop) is str:\n",
    "                for word in prop.split():\n",
    "                    processed_prop += ps.stem(word) + ' '\n",
    "                processed_df.at[0, column] = processed_prop\n",
    "            else:\n",
    "                processed_df.at[0, column] = prop\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47e0872-f8ba-4945-a199-bf2efc3b0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anime_i in range(len(animes)):\n",
    "    \n",
    "    anime = pd.read_csv('C:/Users/marti/Desktop/tsv_files_full/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "\n",
    "    processed_df = pd.DataFrame(columns=names)    \n",
    "    processed_df = removeStopwords(anime)\n",
    "    processed_df = removePunctuation(processed_df)\n",
    "    processed_df = stemming(processed_df)\n",
    "\n",
    "    processed_df.to_csv('C:/Users/marti/Desktop/tsv_processed/anime_{i}.tsv'.format(i=anime_i), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980fa2b-3543-49aa-a1c1-5ed66bef7b5e",
   "metadata": {},
   "source": [
    "### 2.1.1 Create the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c1eab05-9cf5-489a-af81-2223f41bb37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_lst = []\n",
    "for anime_i in range(len(animes)):\n",
    "    anime = pd.read_csv('C:/Users/marti/Desktop/tsv_processed/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "    animes_lst.append(anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcab93ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Title Type  Episodes  Members Score    Users  \\\n",
      "0    fullmet alchemist brotherhood   tv         64  2675751   NaN  1622384   \n",
      "1                               NaN  tv         51   483807   NaN   169476   \n",
      "2  shingeki kyojin season 3 part 2   tv         10  1596039   NaN  1087519   \n",
      "3                       stein gate   tv         24  2090910   NaN  1109700   \n",
      "4           fruit basket the final   tv         13   275214   NaN   113310   \n",
      "\n",
      "  Rank  Popularity                                        Description  \\\n",
      "0    1           3  after horrif alchemi experi goe wrong elric ho...   \n",
      "1    2         337  gintoki shinpachi kagura return broke member y...   \n",
      "2    3          33  restor diminish hope survey corp embark missio...   \n",
      "3    4          11  mad scientist rintar okab rent room ricketi ol...   \n",
      "4    5         651  year ago chines zodiac spirit god swore stay t...   \n",
      "\n",
      "                                             Related  \\\n",
      "0  fullmet alchemist version fullmet alchemist st...   \n",
      "1  gintama gintama movi 2 yorozuya yo eien nare s...   \n",
      "2  shingeki kyojin shingeki kyojin season 3 shing...   \n",
      "3  stein gate set chäo head stein gate oukoubakko...   \n",
      "4  fruit basket fruit basket 2nd season fruit bas...   \n",
      "\n",
      "                                          Characters  \\\n",
      "0  edward alphons roy mae riza ling alex loui win...   \n",
      "1  gintoki shinpachi kotar toushir sougo shinsuk ...   \n",
      "2  eren mikasa armin erwin hang sasha reiner jean...   \n",
      "3  rintar kurisu mayuri itaru suzuha ruka rumiho ...   \n",
      "4  kyou tooru yuki hatsuharu momiji shigur ayam h...   \n",
      "\n",
      "                                              Voices  \\\n",
      "0  romi rie shinichiro keiji yuuichi fumiko mamor...   \n",
      "1  tomokazu rie daisuk akira kazuya kenichi takeh...   \n",
      "2  hiroshi yuki yui marina daisuk romi yuu yoshim...   \n",
      "3  mamoru asami kana tomokazu yukari yuu haruko s...   \n",
      "4  yuuma manaka nobunaga makoto megumi yuuichi ta...   \n",
      "\n",
      "                                               Staff release   end  \n",
      "0  justin noritomo yasuhiro episod director story...    nan   nan   \n",
      "1  youichi storyboard plan chizuru storyboard key...    nan   nan   \n",
      "2                shuuhei jouji katsuji plan tetsuya     nan   nan   \n",
      "3  gaku takeshi plan hiroshi episod director stor...    nan   nan   \n",
      "4  yoshihid jin director up song perform song per...    nan   nan   \n"
     ]
    }
   ],
   "source": [
    "#print(animes_lst)    \n",
    "\n",
    "animes_df = pd.concat(animes_lst)\n",
    "animes_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "animes_df.reset_index(inplace = True, drop=True)\n",
    "print(animes_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e5db8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "inverted_index = {}\n",
    "term_id = 1\n",
    "\n",
    "for index, anime in animes_df.iterrows():\n",
    "\n",
    "    description = anime['Description']  # 'Ciao mi chiamo Roberta'\n",
    "    if type(description) is str:\n",
    "        words = description.split()  # ['ciao', 'mi', 'chiamo', 'Roberta']\n",
    "\n",
    "        for word in words:\n",
    "\n",
    "            # Create vocabulary\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = term_id\n",
    "                term_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83d9a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, anime in animes_df.iterrows():\n",
    "\n",
    "    description = anime['Description']  # 'Ciao mi chiamo Roberta'\n",
    "    if type(description) is str:\n",
    "        words = description.split()  # ['ciao', 'mi', 'chiamo', 'Roberta']\n",
    "\n",
    "        for word in words:\n",
    "            term_id = vocabulary[word]\n",
    "            if term_id not in inverted_index:\n",
    "                inverted_index[term_id] = []\n",
    "            if index not in inverted_index[term_id]:\n",
    "                inverted_index[term_id].append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f2db0",
   "metadata": {},
   "source": [
    "## 2.1.2 Execute query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bb53306",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_original = []\n",
    "for anime_i in range(len(animes)):\n",
    "    anime_original = pd.read_csv('C:/Users/marti/Desktop/tsv_files_full/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "    animes_original.append(anime_original)\n",
    "animes_original = pd.concat(animes_original)\n",
    "animes_original = animes_original.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "85d92a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "animes_original.to_csv('C:/Users/marti/Desktop/file_originali_tsv/anime_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "60405d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list_anime.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e398f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query):\n",
    "    term_id=0\n",
    "    first=True\n",
    "    query=query.split()\n",
    "    q_match=pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "\n",
    "    try:\n",
    "        for term in query:\n",
    "            if term in vocabulary:\n",
    "                term_id=vocabulary[term]\n",
    "            if first:\n",
    "                lst_doc=inverted_index[term_id]\n",
    "                first=False\n",
    "            else:\n",
    "                lst_doc=set(lst_doc).intersection(inverted_index[term_id])\n",
    "                if len(lst_doc)==0:\n",
    "                    print('no doc found')\n",
    "                    break\n",
    "\n",
    "        for i, doc in enumerate(lst_doc):\n",
    "            q_match.at[i, ['Title', 'Description', 'URL']]=animes_original.iloc[doc]['Title'], animes_original.iloc[doc]['Description'], lines[doc]\n",
    "            #q_match.at[i, 'Description']=animes_original.iloc[doc]['Description']\n",
    "            #q_match.at[i, 'URL']=lines[doc]\n",
    "        \n",
    "        display(q_match)\n",
    "    except:\n",
    "        print(\"no doc found\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8f29dea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>[\"After a horrific alchemy experiment goes wro...</td>\n",
       "      <td>https://myanimelist.net/anime/5114/Fullmetal_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gintama.</td>\n",
       "      <td>[\"After joining the resistance against the bak...</td>\n",
       "      <td>https://myanimelist.net/anime/34096/Gintama\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clannad: After Story</td>\n",
       "      <td>[&lt;i&gt;Clannad: After Story&lt;/i&gt;, ', the sequel to...</td>\n",
       "      <td>https://myanimelist.net/anime/4181/Clannad__Af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gintama.: Shirogane no Tamashii-hen</td>\n",
       "      <td>[\"After the fierce battle on Rakuyou, the unto...</td>\n",
       "      <td>https://myanimelist.net/anime/36838/Gintama__S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cowboy Bebop</td>\n",
       "      <td>['In the year 2071, humanity has colonized sev...</td>\n",
       "      <td>https://myanimelist.net/anime/1/Cowboy_Bebop\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>Chu^2</td>\n",
       "      <td>['Wicked Trio 1 - The Affair ', '\\n', '\\r\\nIt\\...</td>\n",
       "      <td>https://myanimelist.net/anime/5570/Binetsu_Him...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>The Satisfaction</td>\n",
       "      <td>['Reiko is tormented by a nightmare of a demon...</td>\n",
       "      <td>https://myanimelist.net/anime/19747/Dakaretai_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>Rhythm: Koi no Rhythm</td>\n",
       "      <td>[\"Hiro is a high school student with a very st...</td>\n",
       "      <td>https://myanimelist.net/anime/5571/Shining_May\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>Kunoichi Gakuen Ninpouchou</td>\n",
       "      <td>[\"As night arrives the shadows of Koga fall on...</td>\n",
       "      <td>https://myanimelist.net/anime/4575/Junk_Story_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>Pico to Chico</td>\n",
       "      <td>['The crisp rays of summer sun find the effemi...</td>\n",
       "      <td>https://myanimelist.net/anime/5160/Wake_Up_Ari...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Title  \\\n",
       "0       Fullmetal Alchemist: Brotherhood   \n",
       "1                               Gintama.   \n",
       "2                   Clannad: After Story   \n",
       "3    Gintama.: Shirogane no Tamashii-hen   \n",
       "4                           Cowboy Bebop   \n",
       "..                                   ...   \n",
       "745                                Chu^2   \n",
       "746                     The Satisfaction   \n",
       "747                Rhythm: Koi no Rhythm   \n",
       "748           Kunoichi Gakuen Ninpouchou   \n",
       "749                        Pico to Chico   \n",
       "\n",
       "                                           Description  \\\n",
       "0    [\"After a horrific alchemy experiment goes wro...   \n",
       "1    [\"After joining the resistance against the bak...   \n",
       "2    [<i>Clannad: After Story</i>, ', the sequel to...   \n",
       "3    [\"After the fierce battle on Rakuyou, the unto...   \n",
       "4    ['In the year 2071, humanity has colonized sev...   \n",
       "..                                                 ...   \n",
       "745  ['Wicked Trio 1 - The Affair ', '\\n', '\\r\\nIt\\...   \n",
       "746  ['Reiko is tormented by a nightmare of a demon...   \n",
       "747  [\"Hiro is a high school student with a very st...   \n",
       "748  [\"As night arrives the shadows of Koga fall on...   \n",
       "749  ['The crisp rays of summer sun find the effemi...   \n",
       "\n",
       "                                                   URL  \n",
       "0    https://myanimelist.net/anime/5114/Fullmetal_A...  \n",
       "1        https://myanimelist.net/anime/34096/Gintama\\n  \n",
       "2    https://myanimelist.net/anime/4181/Clannad__Af...  \n",
       "3    https://myanimelist.net/anime/36838/Gintama__S...  \n",
       "4       https://myanimelist.net/anime/1/Cowboy_Bebop\\n  \n",
       "..                                                 ...  \n",
       "745  https://myanimelist.net/anime/5570/Binetsu_Him...  \n",
       "746  https://myanimelist.net/anime/19747/Dakaretai_...  \n",
       "747   https://myanimelist.net/anime/5571/Shining_May\\n  \n",
       "748  https://myanimelist.net/anime/4575/Junk_Story_...  \n",
       "749  https://myanimelist.net/anime/5160/Wake_Up_Ari...  \n",
       "\n",
       "[750 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query=\"after\"\n",
    "execute_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb8890-8310-491a-af33-41d728e33b30",
   "metadata": {},
   "source": [
    "## 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4eab8-a294-4970-a195-922ac5983fbb",
   "metadata": {},
   "source": [
    "Vogliamo un inverted index dove ad ogni term id sia associata una lista di coppie composte dagli anime in cui è contenuta la parola e il suo TF-IFD score.\n",
    "\n",
    "Il problema sta nel fatto che i dataframe di vocabulary e inverted index andrebbero ritrasformati in dizionari. C'è la funzione DataFrame.to_dict, ma non ho capito come specificare l'header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36dbd4c-51fa-4e3a-ac8a-75202d22a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.read_csv('/home/roberta/dataScience/01/Algorithmic Methods of Data Mining and Laboratory/hw3/vocabulary.csv', header=None)\n",
    "inverted_index = pd.read_csv('/home/roberta/dataScience/01/Algorithmic Methods of Data Mining and Laboratory/hw3/inverted_index.csv', header=None)\n",
    "\"\"\"\n",
    "animes_df = pd.concat(animes_lst)\n",
    "animes_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "animes_df.reset_index(inplace = True, drop=True)\n",
    "\"\"\"\n",
    "animes_df = pd.read_csv('/path')  # dato che l'hai salvato\n",
    "\n",
    "second_inverted_index = {}\n",
    "\n",
    "for index, anime in animes_df.iterrows():\n",
    "\n",
    "    vocabulary_occurrences = {}\n",
    "    description = anime['Description']  # 'Ciao mi chiamo Roberta'\n",
    "    occurrence = 0\n",
    "\n",
    "    if type(description) is str:\n",
    "        words = description.split()  # ['Ciao', 'mi', 'chiamo', 'Roberta']\n",
    "\n",
    "        # For each word in the description\n",
    "        for word in words:\n",
    "\n",
    "            # Fill the vocabulary with the occurrence\n",
    "            vocabulary_occurrences[word] = occurrence + 1\n",
    "            occurrence += 1\n",
    "\n",
    "    tot_n = len(vocabulary_occurrences)\n",
    "    # For each word in the vocabulary with the occurrences\n",
    "    for word in vocabulary_occurrences:\n",
    "        # Take the term id from the already made vocabulary\n",
    "        term_id = vocabulary[word]\n",
    "\n",
    "        # Calculate term frequency = # word appears in doc / # tot words\n",
    "        word_n = vocabulary_occurrences[word]\n",
    "        tf = word_n / tot_n\n",
    "\n",
    "        # Calculate Inverse Data Frequency = log(# tot docs / # doc containing term i\n",
    "        tot_docs = len(animes)\n",
    "        doc_n = len(inverted_index[term_id])\n",
    "        idf = math.log(tot_docs / doc_n, 10)\n",
    "\n",
    "        # Calculate the TF-IDF score\n",
    "        tfIdf = tf * idf\n",
    "\n",
    "        # Fill the second inverted index\n",
    "        if term_id not in second_inverted_index:\n",
    "            second_inverted_index[term_id] = []\n",
    "        if index not in second_inverted_index[term_id]:\n",
    "            second_inverted_index[term_id].append((index, tfIdf))\n",
    "\n",
    "print(second_inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7aa32b",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cabccc",
   "metadata": {},
   "source": [
    "In the given problem, the goal is to choose the maximum number of appointments such as the appointments choosen are not consecutive. It means that the personal trainer want needs a break between appointments and so he can't accept two consecutive request. Obviously, all the request are consider in chronological order. The input data is just a list of the requested appointment, and we want to maximize the sum of value (not adiacent) contained in the list.\n",
    "\n",
    "One solution could be to followed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "797a89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sum(app):\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    tmp1 = 0\n",
    "    tmp2 = 0\n",
    "    for i in range(len(app)):\n",
    "        if ((i % 2) == 0):\n",
    "            tmp1 += app[i]\n",
    "            l1.append(app[i])\n",
    "        else:\n",
    "            tmp2 += app[i]\n",
    "            l2.append(app[i])\n",
    "    if (tmp1 > tmp2):\n",
    "        return l1, tmp1\n",
    "    else:\n",
    "        return l2, tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7aa4c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appointments required: [30, 40, 25, 50, 30, 20] \n",
      "\n",
      "List of appointment accepted:  [40, 50, 20]\n",
      "total duration (of the accepted appointment):  110\n"
     ]
    }
   ],
   "source": [
    "app = [30, 40, 25, 50, 30, 20] \n",
    "print('Appointments required:', app, '\\n') \n",
    "\n",
    "lst, duration=max_sum(app)\n",
    "print('List of appointment accepted: ', lst)\n",
    "print('total duration (of the accepted appointment): ', duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dcfc74",
   "metadata": {},
   "source": [
    "In this program we observe each element of the list twice (one from the even index, and one from the odd index), both of this going through the list with a step of 2. Thenwe add the odd and even values ​​respectively, and then compare them. At the end, we choose the path that maximize the sum."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
