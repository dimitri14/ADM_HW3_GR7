{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64f13d2",
   "metadata": {},
   "source": [
    "# Homework 3 - What is the best anime in the world?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3a15d",
   "metadata": {},
   "source": [
    "- Martina Milazzo\n",
    "- Dimitri Saliola\n",
    "- Roberta Giorgi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b61cb8",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74c803",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 383/383 [05:05<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#383 pages * 50 animes (19130 total)\n",
    "\n",
    "with open(\"C:/Users/marti/Desktop/HW3/list_anime.txt\", \"w\", encoding='utf-8') as file:\n",
    "    \n",
    "    for page in tqdm(range(0, 383)):\n",
    "    \n",
    "        url = 'https://myanimelist.net/topanime.php?limit=' + str(page*50) #single page URL\n",
    "        response = r.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser') #get html\n",
    "\n",
    "        #extract all the links\n",
    "        for tag in soup.find_all(\"tr\", class_=\"ranking-list\"):\n",
    "            a_list = tag.find_all('a', class_=\"hoverinfo_trigger fl-l ml12 mr8\" ,href=True)\n",
    "            for a in a_list:\n",
    "                link = a['href']\n",
    "                file.write(str(link) + '\\n')\n",
    "                \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e0d6f",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXECUTE THIS CELL JUST ONES#\n",
    "\n",
    "file = open(\"C:/Users/marti/Desktop/HW3/list_anime.txt\", \"r\", encoding='utf-8')\n",
    "\n",
    "pages_dir=\"C:/Users/marti/Desktop/HW3/Pages\"\n",
    "\n",
    "if not os.path.exists(pages_dir):\n",
    "    os.makedirs(pages_dir)\n",
    "\n",
    "counter_anime = 855\n",
    "page = 18\n",
    "\n",
    "for line in file:\n",
    "\n",
    "    counter_anime += 1\n",
    "    if (counter_anime%50 == 1):\n",
    "        page +=1\n",
    "\n",
    "    request = r.get(line, headers={'Cache-Control': 'no-cache'})\n",
    "    if str(request.status_code)[0] == '4': \n",
    "        while(True): \n",
    "            time.sleep(120)\n",
    "            request = r.get(line, headers={'Cache-Control': 'no-cache'})\n",
    "            print(request.status_code)\n",
    "            if str(request.status_code)[0] == '2': \n",
    "                break       \n",
    "            \n",
    "\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "    folder_dir=pages_dir+\"/Page\"+str(page)\n",
    "    if not os.path.exists(folder_dir):\n",
    "        os.makedirs(folder_dir)\n",
    "\n",
    "    txt_dir=folder_dir + \"/article_\" + str(counter_anime)+ \".html\"\n",
    "    \n",
    "    with open(txt_dir, \"w\", encoding='utf-8') as file:\n",
    "        file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "729d9b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19131\n"
     ]
    }
   ],
   "source": [
    "# Count anime links in file\n",
    "\n",
    "file = open(\"C:\\\\Users\\\\marti\\\\Desktop\\\\HW3\\\\list_anime.txt\", \"r\", encoding='utf-8')\n",
    "line_count = 0\n",
    "for line in file:\n",
    "    if line != \"\\n\":\n",
    "        line_count += 1\n",
    "file.close()\n",
    "\n",
    "print(line_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0048d",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac451974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXECUTE THIS CELL JUST ONES#\n",
    "\n",
    "title = []\n",
    "types = []\n",
    "numEpisode = []\n",
    "release = []\n",
    "end = []\n",
    "numMembers = []\n",
    "score = []\n",
    "users = []\n",
    "rank = []\n",
    "popularity = []\n",
    "description = []\n",
    "related = []\n",
    "characters = []\n",
    "voices = []\n",
    "staff = []\n",
    "\n",
    "pages = os.listdir(\"C:/Users/marti/Desktop/HW3/Pages\")[1:]\n",
    "\n",
    "for p in pages:\n",
    "    n_html = os.listdir(pages+\"/\"+str(p))\n",
    "    for i in range(1, len(n_html)+1):\n",
    "        file = open(pages+\"/\"+str(p)+\"/article_\"+str(i), 'r', encoding=\"utf8\")\n",
    "        anime = BeautifulSoup(file, 'lxml')\n",
    "        \n",
    "        #TITLE\n",
    "        try:\n",
    "            t=anime.strong.contents[0]\n",
    "            title.append(t)\n",
    "        except:\n",
    "            title.append('NA')\n",
    "            \n",
    "\n",
    "        #TYPE\n",
    "        try:\n",
    "            ty=anime.find(text = 'Type:').find_next('a').contents[0]\n",
    "            types.append(ty)\n",
    "        except:\n",
    "            types.append('NA')\n",
    "\n",
    "\n",
    "        #NUM_EPISODE\n",
    "        try:\n",
    "            n=int(anime.find(text = 'Episodes:').next_element.strip())\n",
    "            numEpisode.append(n)\n",
    "        except:\n",
    "            numEpisode.append(n)\n",
    "\n",
    "\n",
    "        #RELEASE_DATE\n",
    "        try:\n",
    "            date = anime.find(text = 'Aired:').next_element.strip()\n",
    "            if len(date)>15:\n",
    "                rl=datetime.strptime(date.split(\" to \")[0], '%b %d, %Y' )\n",
    "            else:\n",
    "                rl=datetime.strptime(date, '%b %d, %Y' )\n",
    "            release.append(rl)\n",
    "        except:\n",
    "            release.append('NA')\n",
    "\n",
    "\n",
    "        #END_DATE\n",
    "        try:\n",
    "            date = anime.find(text = 'Aired:').next_element.strip()\n",
    "            if len(date)>15 and date.split(\" to \")[1] != \"?\":\n",
    "                e=datetime.strptime(date.split(\" to \")[1], '%b %d, %Y' )\n",
    "            else:\n",
    "                e=pd.to_datetime(np.NaN, errors='coerce')\n",
    "            end.append(e)\n",
    "        except:\n",
    "            end.append('NA')\n",
    "\n",
    "\n",
    "        #NUM_MEMBER\n",
    "        try:\n",
    "            num_mem = anime.find(text = 'Members:').next_element\n",
    "            nm=int(num_mem.replace('n', '').replace(',', '').strip())\n",
    "            numMembers.append(nm)\n",
    "        except:\n",
    "            numMembers.append(0)\n",
    "\n",
    "\n",
    "        #SCORE\n",
    "        try:\n",
    "            s=anime.find(text = 'Score:').find_next('span').contents\n",
    "            s=float(s[0])\n",
    "            score.append(s)\n",
    "        except:\n",
    "            score.append(None)\n",
    "\n",
    "\n",
    "        #USERS\n",
    "        try:\n",
    "            us = anime.find(text = 'Score:').find_next('span').find_next('span').contents\n",
    "            u=int(us[0])\n",
    "            users.append(u)\n",
    "        except:\n",
    "            users.append(0)\n",
    "\n",
    "\n",
    "        #RANK\n",
    "        try:\n",
    "            rk = anime.find(text = 'Ranked:').next_element\n",
    "            rk=int(rk.replace('\\n', '').replace('#', '').strip())\n",
    "            rank.append(rk)\n",
    "        except:\n",
    "            rank.append(None)\n",
    "\n",
    "\n",
    "        #POPULARITY\n",
    "        try:\n",
    "            pop = anime.find(text='Popularity:').next_element\n",
    "            pop=int(pop.replace(\"\\n\",\"\").replace('#', '').strip())\n",
    "            popularity.append(pop)\n",
    "        except:\n",
    "            popularity.append(None)\n",
    "\n",
    "\n",
    "        #DESCRIPTION\n",
    "        try:\n",
    "            des=anime.find(text = 'Synopsis:').find_next('p').text\n",
    "            des=des.replace(\"\\n\",\"\")\n",
    "            description.append(des)\n",
    "        except:\n",
    "            description.append('NA')\n",
    "\n",
    "\n",
    "        #RELATED\n",
    "        list = anime.find(text = 'Related Anime')\n",
    "        rel = []\n",
    "\n",
    "        if(list != None):\n",
    "\n",
    "            try:    \n",
    "                list = list.find_next('list')\n",
    "                list = list.find_all('a')\n",
    "\n",
    "                for t in list:\n",
    "                    rel.append(t.text)\n",
    "\n",
    "            except:\n",
    "                for t in list:\n",
    "                    rel.append('NA')\n",
    "\n",
    "        related.append(rel)\n",
    "\n",
    "        #CHARACTER\n",
    "        c = []\n",
    "        try:\n",
    "            list = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "            list = list.find_all('list')\n",
    "\n",
    "            for t in list:\n",
    "                people = t.find_all('h3')\n",
    "                for p in people:\n",
    "                    c.append(p.text)\n",
    "        except:\n",
    "            c.append('NA')\n",
    "            \n",
    "        characters.append(c)\n",
    "\n",
    "\n",
    "        #VOICES\n",
    "        v=[]\n",
    "        try:\n",
    "            list = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "            list = list.find_all('h3')\n",
    "\n",
    "            for t in list:\n",
    "                people = el.find_next('list')\n",
    "                for person in people:\n",
    "                    v.append(person.find('a').text)\n",
    "        except:\n",
    "            v.append('NA')\n",
    "\n",
    "        voices.append(v)\n",
    "\n",
    "        #STAFF\n",
    "        st = []\n",
    "        try:\n",
    "            list = anime.find_all(text = 'Staff')[1].find_next(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "            \n",
    "            if(list != None):    \n",
    "                table = list.find_all(\"table\")\n",
    "                for t in table:\n",
    "                    i = t.find_all(\"td\")[1]\n",
    "                    p = [i.find(\"a\").text, i.find(\"small\").text]\n",
    "                    st.append(p)\n",
    "        except:\n",
    "            st.append(p)\n",
    "\n",
    "        staff.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b45399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXECUTE THIS CELL JUST ONES#\n",
    "\n",
    "col = ['animeTitle', 'animeType', 'animeNumEpisode','releaseDate', 'endDate', 'animeNumMembers', 'animeScore', 'animeUsers', \n",
    "       'animeRank', 'animePopularity', 'animdeDescription', 'animeRelated', 'animeCharacters', 'animeVoices', 'animeStaff']\n",
    "\n",
    "types = {'animeTitle' : 'object', \n",
    "         'animeType' : 'object', \n",
    "         'animeNumEpisode' : 'int64',\n",
    "         'releaseDate' : 'datetime64', \n",
    "         'endDate' : 'datetime64', \n",
    "         'animeNumMembers' : 'int64', \n",
    "         'animeScore' : 'float64',\n",
    "         'animeUsers' : 'int64', \n",
    "         'animeRank' : 'int64',\n",
    "         'animePopularity' : 'int64',\n",
    "         'animdeDescription' : 'object',\n",
    "         'animeRelated' : 'object',\n",
    "         'animeCharacters' : 'object',\n",
    "         'animeVoices' : 'object',\n",
    "         'animeStaff' : 'object'}\n",
    "\n",
    "data = list(zip(title,types,numEpisode,release,end,numMembers,score,users,rank,popularity,description,related,characters,voices,staff))\n",
    "\n",
    "dataset = pd.DataFrame(data, columns = col).astype(dtype = types)  \n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    with open('C:/Users/marti/Desktop/HW3/tsv/anime_'+str(i)+'.tsv', 'w') as file:\n",
    "        tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "        tsv_writer.writerow([x for x in dataset.columns]) \n",
    "        tsv_writer.writerow(x for x in dataset.iloc[i]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac4605-7169-4017-9fca-a4be171fee2d",
   "metadata": {},
   "source": [
    "# 2. Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae83e1c0-baca-4144-a926-8d996e42ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "\n",
    "# Get a list of the animes\n",
    "animes = os.listdir('C:/Users/marti/Desktop/tsv_files_full')\n",
    "\n",
    "# Save column names\n",
    "names = ['Title', 'Type', 'Episodes', 'Members', 'Score', 'Users', 'Rank', 'Popularity', 'Description', 'Related', 'Characters', 'Voices', 'Staff', 'release', 'end']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1f769-c58b-4c10-9dc5-c91ea53e380c",
   "metadata": {},
   "source": [
    "### 2.0.1 Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e802475-ee90-4ec9-999c-5554c1cf5876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') #library for removing stopwords in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "298db704-bd30-4f01-b69f-551a62a12bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(anime):\n",
    "    \n",
    "    # Save the English stopwords in a variable\n",
    "    en_stops = set(stopwords.words('english'))\n",
    "    \n",
    "    for column in anime:\n",
    "        processed_prop = ''\n",
    "        for prop in anime[column]:\n",
    "            if type(prop) is str:\n",
    "                for word in prop.split():\n",
    "                    if word not in en_stops: #if the word isn't a stopword, it will be stored in the \"processed_prop\" variable\n",
    "                        processed_prop += word + ' '\n",
    "                processed_df.at[0, column] = processed_prop\n",
    "            else:\n",
    "                processed_df.at[0, column] = prop\n",
    "                \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d10f13-7ada-44c0-b385-d5ef8802e36f",
   "metadata": {},
   "source": [
    "### 2.0.2 Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e9ff906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') #library to remove the punctuation in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c662063-b5c1-4022-a61f-424f4813e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(anime):\n",
    "    \n",
    "    for column in anime:\n",
    "        for prop in anime[column]:\n",
    "            words = nltk.word_tokenize(str(prop))\n",
    "            processed = [word for word in words if word.isalnum()]  #if the word isn't a punctuation it will be stored in \"proccessed\" variable\n",
    "            processed_df.at[0, column] = ' '.join(processed)\n",
    "            \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e765e8-65af-480c-800d-0452243204f4",
   "metadata": {},
   "source": [
    "### 2.0.3 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ddfb9b7-501b-499e-940e-23f520a41df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(anime):\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for column in anime:\n",
    "        processed_prop = ''\n",
    "        for prop in anime[column]:\n",
    "            if type(prop) is str:\n",
    "                for word in prop.split():\n",
    "                    #for each word, we apply the Porter's Stemmers algorithm, in which we truncate each word to extract the radix\n",
    "                    processed_prop += ps.stem(word) + ' '       \n",
    "                processed_df.at[0, column] = processed_prop\n",
    "            else:\n",
    "                processed_df.at[0, column] = prop\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e47e0872-f8ba-4945-a199-bf2efc3b0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anime_i in range(len(animes)):\n",
    "    \n",
    "    anime = pd.read_csv('C:/Users/marti/Desktop/tsv_files_full/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "\n",
    "    processed_df = pd.DataFrame(columns=names)    \n",
    "    processed_df = removeStopwords(anime)\n",
    "    processed_df = removePunctuation(processed_df)\n",
    "    processed_df = stemming(processed_df)\n",
    "\n",
    "    processed_df.to_csv('C:/Users/marti/Desktop/tsv_processed/anime_{i}.tsv'.format(i=anime_i), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980fa2b-3543-49aa-a1c1-5ed66bef7b5e",
   "metadata": {},
   "source": [
    "### 2.1.1 Create the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c1eab05-9cf5-489a-af81-2223f41bb37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with the original .tsv files \n",
    "animes_lst = []\n",
    "for anime_i in range(len(animes)):\n",
    "    anime = pd.read_csv('C:/Users/marti/Desktop/tsv_processed/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "    animes_lst.append(anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcab93ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Title Type  Episodes  Members Score    Users  \\\n",
      "0    fullmet alchemist brotherhood   tv         64  2675751   NaN  1622384   \n",
      "1                               NaN  tv         51   483807   NaN   169476   \n",
      "2  shingeki kyojin season 3 part 2   tv         10  1596039   NaN  1087519   \n",
      "3                       stein gate   tv         24  2090910   NaN  1109700   \n",
      "4           fruit basket the final   tv         13   275214   NaN   113310   \n",
      "\n",
      "  Rank  Popularity                                        Description  \\\n",
      "0    1           3  after horrif alchemi experi goe wrong elric ho...   \n",
      "1    2         337  gintoki shinpachi kagura return broke member y...   \n",
      "2    3          33  restor diminish hope survey corp embark missio...   \n",
      "3    4          11  mad scientist rintar okab rent room ricketi ol...   \n",
      "4    5         651  year ago chines zodiac spirit god swore stay t...   \n",
      "\n",
      "                                             Related  \\\n",
      "0  fullmet alchemist version fullmet alchemist st...   \n",
      "1  gintama gintama movi 2 yorozuya yo eien nare s...   \n",
      "2  shingeki kyojin shingeki kyojin season 3 shing...   \n",
      "3  stein gate set chäo head stein gate oukoubakko...   \n",
      "4  fruit basket fruit basket 2nd season fruit bas...   \n",
      "\n",
      "                                          Characters  \\\n",
      "0  edward alphons roy mae riza ling alex loui win...   \n",
      "1  gintoki shinpachi kotar toushir sougo shinsuk ...   \n",
      "2  eren mikasa armin erwin hang sasha reiner jean...   \n",
      "3  rintar kurisu mayuri itaru suzuha ruka rumiho ...   \n",
      "4  kyou tooru yuki hatsuharu momiji shigur ayam h...   \n",
      "\n",
      "                                              Voices  \\\n",
      "0  romi rie shinichiro keiji yuuichi fumiko mamor...   \n",
      "1  tomokazu rie daisuk akira kazuya kenichi takeh...   \n",
      "2  hiroshi yuki yui marina daisuk romi yuu yoshim...   \n",
      "3  mamoru asami kana tomokazu yukari yuu haruko s...   \n",
      "4  yuuma manaka nobunaga makoto megumi yuuichi ta...   \n",
      "\n",
      "                                               Staff release   end  \n",
      "0  justin noritomo yasuhiro episod director story...    nan   nan   \n",
      "1  youichi storyboard plan chizuru storyboard key...    nan   nan   \n",
      "2                shuuhei jouji katsuji plan tetsuya     nan   nan   \n",
      "3  gaku takeshi plan hiroshi episod director stor...    nan   nan   \n",
      "4  yoshihid jin director up song perform song per...    nan   nan   \n"
     ]
    }
   ],
   "source": [
    "animes_df = pd.concat(animes_lst)\n",
    "animes_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "animes_df.reset_index(inplace = True, drop=True)\n",
    "print(animes_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e5db8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary in which for each key = word and for each values = term_id, a univoque index for refearing at the word\n",
    "def create_vocabulary(dataframe):\n",
    "    vocabulary = {}\n",
    "    term_id = 1\n",
    "\n",
    "    for index, anime in dataframe.iterrows():\n",
    "\n",
    "        description = anime['Description']  \n",
    "        if type(description) is str:\n",
    "            words = description.split()  \n",
    "\n",
    "            for word in words:\n",
    "\n",
    "                # Create vocabulary\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = term_id\n",
    "                    term_id += 1\n",
    "          \n",
    "    return vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83d9a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an inverted index, in which for all the term_id of all the word we store all the document in which appear the word\n",
    "def create_inverted_index(vocabulary, dataframe):\n",
    "    inverted_index={}\n",
    "    for index, anime in dataframe.iterrows():\n",
    "\n",
    "        description = anime['Description']  \n",
    "        if type(description) is str:\n",
    "            words = description.split()  \n",
    "\n",
    "            for word in words:\n",
    "                term_id = vocabulary[word]\n",
    "                if term_id not in inverted_index:\n",
    "                    inverted_index[term_id] = []\n",
    "                if index not in inverted_index[term_id]:\n",
    "                    inverted_index[term_id].append(index)\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35905078",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=create_vocabulary(animes_df)\n",
    "inverted_index=create_inverted_index(vocabulary, animes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f2db0",
   "metadata": {},
   "source": [
    "## 2.1.2 execute query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bb53306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary to return the original description of the anime (not the processed one)\n",
    "animes_original = []\n",
    "for anime_i in range(len(animes)):\n",
    "    anime_original = pd.read_csv('C:/Users/marti/Desktop/tsv_files_full/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "    animes_original.append(anime_original)\n",
    "animes_original = pd.concat(animes_original)\n",
    "animes_original = animes_original.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85d92a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "animes_original.to_csv('C:/Users/marti/Desktop/file_originali_tsv/anime_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60405d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary to get the urls for each anime\n",
    "with open('C:/Users/marti/Desktop/HW3/list_anime.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e398f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the goal of this function is to return all the document in which appears all the term passed in the string \"query\"\n",
    "def execute_query(query):\n",
    "    term_id=0\n",
    "    first=True\n",
    "    query=query.split()\n",
    "    q_match=pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "\n",
    "    try:\n",
    "        for term in query:\n",
    "            if term in vocabulary:\n",
    "                term_id=vocabulary[term]\n",
    "            if first:\n",
    "                lst_doc=inverted_index[term_id]\n",
    "                first=False\n",
    "            else:\n",
    "                lst_doc=set(lst_doc).intersection(inverted_index[term_id])\n",
    "                if len(lst_doc)==0:\n",
    "                    print('no doc found')\n",
    "                    break\n",
    "\n",
    "        for i, doc in enumerate(lst_doc):\n",
    "            q_match.at[i, ['Title', 'Description', 'URL']]=animes_original.iloc[doc]['Title'], animes_original.iloc[doc]['Description'], lines[doc]\n",
    "        \n",
    "        display(q_match)\n",
    "    except:\n",
    "        print(\"no doc found\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f29dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to execute correctly the execute_query() function we have to preprocess the string given in input\n",
    "def pre_process_query(query):\n",
    "    tokenized_query = nltk.word_tokenize(query)\n",
    "    en_stops = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    # Process the query before executing it\n",
    "    processed_final=\" \"\n",
    "\n",
    "    remove_punctuation = [t for t in tokenized_query if t.isalnum()]\n",
    "    \n",
    "    remove_stopwords = [t for t in remove_punctuation if t not in en_stops]\n",
    "    \n",
    "    \n",
    "    for word in remove_stopwords:\n",
    "        processed_final +=ps.stem(word)+\" \"\n",
    "\n",
    "    \n",
    "    \n",
    "    return processed_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea4663af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " home live hous comput text keyboard phone \n"
     ]
    }
   ],
   "source": [
    "query=\"home lives house computing texting keyboard phone\"\n",
    "proc=pre_process_query(query)\n",
    "print(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf60158",
   "metadata": {},
   "source": [
    "## 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f58ae",
   "metadata": {},
   "source": [
    "### 2.2.1) inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b590932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this inverted_index we add for each document the tf_idf value, an index to to measure the importance of a term with respect to a document or collection of documents\n",
    "#so for each term_id we have the doc number and the tf_idf -> term_id : (doc, tf_idf)\n",
    "def create_second_inverted_index(vocabulary, inverted_index, dataframe):\n",
    "\n",
    "    second_inverted_index={}\n",
    "    \n",
    "    for index, anime in dataframe.iterrows():\n",
    "        vocabulary_occurrences = {}\n",
    "        occurrence=[]\n",
    "\n",
    "        description = anime['Description']  \n",
    "        if type(description) is str:\n",
    "            words = description.split()  \n",
    "\n",
    "            for word in words:\n",
    "                if word not in occurrence:\n",
    "                    vocabulary_occurrences[word]=words.count(word)\n",
    "\n",
    "\n",
    "            tot_words_in_doc=len(words)\n",
    "\n",
    "            for word in vocabulary_occurrences:\n",
    "                term_id=vocabulary[word]\n",
    "\n",
    "                #term frequency=#word appears in doc / #tot words in doc\n",
    "                word_occur=vocabulary_occurrences[word]\n",
    "                tf=word_occur/tot_words_in_doc\n",
    "\n",
    "                #inverse data frequency= log(#tot_docs/#docs_containing_term_i)\n",
    "                tot_docs=len(dataframe)\n",
    "                docs_with_word=len(inverted_index[term_id])\n",
    "                idf=math.log(tot_docs/docs_with_word, 10)\n",
    "\n",
    "                #tf-idf\n",
    "                tf_idf=tf*idf\n",
    "\n",
    "                if term_id not in second_inverted_index:\n",
    "                    second_inverted_index[term_id] = []\n",
    "                if index not in second_inverted_index[term_id]:\n",
    "                    second_inverted_index[term_id].append((index, tf_idf))\n",
    "\n",
    "    return second_inverted_index\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95a7f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = create_vocabulary(animes_df)\n",
    "inverted_index = create_inverted_index(vocabulary, animes_df)\n",
    "\n",
    "second_inverted_index=create_second_inverted_index(vocabulary, inverted_index, animes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8302be",
   "metadata": {},
   "source": [
    "### 2.2.2) Execute query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0448d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function convert all the document in a list of his term_id\n",
    "# doc : term_id  for all the term_id in doc\n",
    "def documents_to_word(dataframe, doc, vocabulary):\n",
    "    vocabulary_occurrences={}\n",
    "    occurrence=[]\n",
    "    description=dataframe.iloc[doc]['Description']\n",
    "    norm1=0\n",
    "    description=description.split()\n",
    "\n",
    "    \n",
    "    for word in description:\n",
    "\n",
    "        if word not in occurrence:\n",
    "            vocabulary_occurrences[word]=description.count(word)\n",
    "\n",
    "\n",
    "\n",
    "    tot_words_in_doc=len(description)\n",
    "\n",
    "    for word in vocabulary_occurrences:\n",
    "        term_id=vocabulary[word]\n",
    "\n",
    "        #term frequency=#word appears in doc / #tot words in doc\n",
    "        word_occur=vocabulary_occurrences[word]\n",
    "        tf=word_occur/tot_words_in_doc\n",
    "\n",
    "        #inverse data frequency= log(#tot_docs/#docs_containing_term_i)\n",
    "        tot_docs=len(dataframe)\n",
    "        docs_with_word=len(inverted_index[term_id])\n",
    "        idf=math.log(tot_docs/docs_with_word, 10)\n",
    "\n",
    "        #tf-idf\n",
    "        tf_idf=tf*idf        \n",
    "        \n",
    "        norm1 += np.power(tf_idf, 2)\n",
    "\n",
    "    return norm1\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e18befb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07765361421590745\n"
     ]
    }
   ],
   "source": [
    "dict_documents=documents_to_word(animes_df, 1, vocabulary) \n",
    "print(dict_documents)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8db1dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calculate the cosine similarity between the query and the description in all the documents\n",
    "def cosine_similarity(query):\n",
    "\n",
    "    query=pre_process_query(query)\n",
    "    cos_similarities = []\n",
    "    ti_query={}\n",
    "    q_match=pd.DataFrame(columns=['Title', 'Description', 'Similarity'])\n",
    "    query=query.split()\n",
    "    try:\n",
    "        for word in query:\n",
    "            tf=query.count(word)/len(query)\n",
    "            term_id=vocabulary[word]\n",
    "            idf=np.log(len(second_inverted_index)/len(second_inverted_index[term_id]))\n",
    "            ti_query[word]=tf*idf\n",
    "\n",
    "\n",
    "        lst_doc = []\n",
    "        for term in query:\n",
    "            if term in vocabulary:\n",
    "                term_id = vocabulary[term]\n",
    "                lst_doc.append(inverted_index[term_id])\n",
    "            else:\n",
    "                print(\"no doc found\")\n",
    "                return\n",
    "\n",
    "        set_doc=lst_doc[0]\n",
    "        for x in lst_doc:\n",
    "            set_doc=set(set_doc).intersection(set(x))\n",
    "\n",
    "\n",
    "        set_doc=sorted(set_doc)\n",
    "        set_doc=list(set_doc)\n",
    "\n",
    "        #denominator 2\n",
    "        norm2 = 0\n",
    "\n",
    "        for q in query:\n",
    "            i = ti_query[q]\n",
    "            norm2 += np.power(i, 2)\n",
    "        norm2 = np.sqrt(norm2)\n",
    "\n",
    "\n",
    "        for term in query:\n",
    "            term_id=vocabulary[term]\n",
    "        \n",
    "            \n",
    "            #scalar product between tf_idf * #number of occurrences of the term in the query \n",
    "            for i, doc in enumerate(set_doc):\n",
    "                c = 0\n",
    "                tf_idf=second_inverted_index[term_id][i][1]\n",
    "                c +=  tf_idf * ti_query[term]\n",
    "\n",
    "                #denominator 1\n",
    "                \n",
    "                norm1=documents_to_word(animes_df, doc, vocabulary) \n",
    "\n",
    "\n",
    "                norm1 = np.sqrt(norm1)\n",
    "\n",
    "\n",
    "                tot=c / (norm1 * norm2)\n",
    "                cos_similarities.append(tot)\n",
    "\n",
    "        \n",
    "\n",
    "        for i, doc in enumerate(set_doc):\n",
    "            q_match.at[i, ['Doc', 'Title', 'Description', 'URL', 'Similarity']] = doc, animes_original.iloc[doc]['Title'], animes_original.iloc[doc]['Description'], lines[doc], cos_similarities[i]\n",
    "            q_match.sort_values(by=['Similarity'], ascending=False, inplace=True)\n",
    "            q_match.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        return q_match\n",
    "    except:\n",
    "        print(\"no doc found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3366c68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Similarity</th>\n",
       "      <th>Doc</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>[\"Bardock, Son Goku's father, is a low-ranking...</td>\n",
       "      <td>0.24986</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>[\"Forty-one years ago on Planet Vegeta, home o...</td>\n",
       "      <td>0.15304</td>\n",
       "      <td>400.0</td>\n",
       "      <td>https://myanimelist.net/anime/1364/Detective_C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>[\"Five years after winning the World Martial A...</td>\n",
       "      <td>0.097484</td>\n",
       "      <td>364.0</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "      <td>[\"Five years after the events of Dragon Ball, ...</td>\n",
       "      <td>0.079373</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>https://myanimelist.net/anime/31483/Akagami_no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "1                           Dragon Ball Super: Broly   \n",
       "2                                      Dragon Ball Z   \n",
       "3                                    Dragon Ball Kai   \n",
       "\n",
       "                                         Description Similarity     Doc  \\\n",
       "0  [\"Bardock, Son Goku's father, is a low-ranking...    0.24986  1467.0   \n",
       "1  [\"Forty-one years ago on Planet Vegeta, home o...    0.15304   400.0   \n",
       "2  [\"Five years after winning the World Martial A...   0.097484   364.0   \n",
       "3  [\"Five years after the events of Dragon Ball, ...   0.079373  1035.0   \n",
       "\n",
       "                                                 URL  \n",
       "0  https://myanimelist.net/anime/986/Dragon_Ball_...  \n",
       "1  https://myanimelist.net/anime/1364/Detective_C...  \n",
       "2  https://myanimelist.net/anime/813/Dragon_Ball_Z\\n  \n",
       "3  https://myanimelist.net/anime/31483/Akagami_no...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(\"saiyan race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db38791",
   "metadata": {},
   "source": [
    "## 3. Define a new score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1a15a",
   "metadata": {},
   "source": [
    "#### Custom score = cos.similarity * 0.4 + type * 0.4 + episodes * 0.1 + popularity * 0.1\n",
    "\n",
    "This index is based on fuor different concepts, the score is higher when the query is more similar to the document, the score increases when the type of the anime match with respect to the type chosen by the users, other important point is that to many people don't like series that are too long, so it's important to increase the score when the number of episodes chosen by the users and those of the dataset and at the end we all know how important the fashions are therefore we want our index to be higher if user's popularity match with our popukarity's range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cc2322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq as hq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71fbd256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the new_score as described above\n",
    "def new_score(dataframe, query, anime_type, episode_min, episode_max, pop_min, pop_max, vocabulary):\n",
    "    df_cos=cosine_similarity(query)\n",
    "    query=pre_process_query(query)\n",
    "    query=query.split()\n",
    "    lst_new_score={}\n",
    "    for index, anime in dataframe.iterrows():\n",
    "        lst_doc=[]\n",
    "        for term in query:\n",
    "            if term in vocabulary:\n",
    "                term_id = vocabulary[term]\n",
    "                lst_doc.append(inverted_index[term_id])\n",
    "            else:\n",
    "                print(\"no doc found - new score\")\n",
    "                return\n",
    "\n",
    "                \n",
    "    set_doc=lst_doc[0]\n",
    "    for x in lst_doc:\n",
    "        set_doc=set(set_doc).intersection(set(x))\n",
    "\n",
    "\n",
    "    set_doc=sorted(set_doc)\n",
    "    set_doc=list(set_doc)\n",
    "\n",
    "    for doc in set_doc:\n",
    "        row=df_cos.loc[df_cos['Doc']==doc]\n",
    "        cos_sim=row.iloc[0][\"Similarity\"]\n",
    "        cos_sim=float(cos_sim)\n",
    "\n",
    "        t_anime=0\n",
    "        n_episode=0\n",
    "        popularity_anime=0\n",
    "\n",
    "        if(anime['Type'] and anime['Type']==anime_type): t_anime=1.0\n",
    "        if(anime['Episodes']>= episode_min and anime['Episodes']<= episode_max): n_episode=1.0 \n",
    "        if(anime['Popularity']>= pop_min and anime['Popularity']<= pop_max): popularity_anime=1.0\n",
    "\n",
    "        my_score=cos_sim * 0.4 + t_anime * 0.4 + n_episode * 0.1 + popularity_anime * 0.1\n",
    "        lst_new_score[doc]=my_score\n",
    "\n",
    "\n",
    "    heap = [(-value, key) for key,value in lst_new_score.items()]\n",
    "    largest = hq.nsmallest(10, heap)\n",
    "    largest = [(key, -value) for value, key in largest]\n",
    "    \n",
    "    matching=dict(largest)\n",
    "\n",
    "    q_match=pd.DataFrame()\n",
    "    for i, doc in enumerate(matching.keys()):\n",
    "        q_match.at[i, ['Doc', 'Title', 'Description', 'URL', 'Similarity']] = int(doc), animes_original.iloc[doc]['Title'], animes_original.iloc[doc]['Description'], lines[doc], matching[doc]\n",
    "        q_match.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    display(q_match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f18c472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to insert the query from input\n",
    "def query_users():\n",
    "\n",
    "    print(\"insert a query: \")\n",
    "    query=input()\n",
    "    print(query)\n",
    "    \n",
    "    print(\"What type of anime do you want?\")\n",
    "    anime_type=input()\n",
    "    print(anime_type)\n",
    "\n",
    "    print(\"How many episode minimum do you want?\")\n",
    "    episode_min=input()\n",
    "    print(episode_min)\n",
    "\n",
    "    print(\"How many episode maximum do you want?\")\n",
    "    episode_max=input()\n",
    "    print(episode_max)\n",
    "\n",
    "    print(\"What popularity minimum do you want?\")\n",
    "    pop_min=input()\n",
    "    print(pop_min)\n",
    "\n",
    "    print(\"What popularity maximum do you want?\")\n",
    "    pop_max=input()\n",
    "    print(pop_max)\n",
    "\n",
    "\n",
    "    vocabulary = create_vocabulary(animes_df)\n",
    "    n_score=new_score(animes_df, query, anime_type, int(episode_min), int(episode_max), int(pop_min), int(pop_max), vocabulary)\n",
    "\n",
    "    return n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60d7d11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert a query: \n",
      "dragon ball\n",
      "What type of anime do you want?\n",
      "tv\n",
      "How many episode minimum do you want?\n",
      "1\n",
      "How many episode maximum do you want?\n",
      "1000\n",
      "What popularity minimum do you want?\n",
      "1\n",
      "What popularity maximum do you want?\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5985.0</td>\n",
       "      <td>Dragon Ball GT: Gokuu Gaiden! Yuuki no Akashi ...</td>\n",
       "      <td>[\"Years after the end of the Dragonball GT, th...</td>\n",
       "      <td>https://myanimelist.net/anime/37422/Cinderella...</td>\n",
       "      <td>0.143356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>364.0</td>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>[\"Five years after winning the World Martial A...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z\\n</td>\n",
       "      <td>0.136394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6557.0</td>\n",
       "      <td>Galo Sengen</td>\n",
       "      <td>['Galo Sengen (\"Galish Man GAL\" in English / G...</td>\n",
       "      <td>https://myanimelist.net/anime/40526/Dragon_Ie_...</td>\n",
       "      <td>0.097717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2267.0</td>\n",
       "      <td>Dragon Ball Z Movie 13: Ryuuken Bakuhatsu!! Go...</td>\n",
       "      <td>['The Z Warriors discover an unopenable music ...</td>\n",
       "      <td>https://myanimelist.net/anime/26349/Danna_ga_N...</td>\n",
       "      <td>0.046869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5275.0</td>\n",
       "      <td>Dragon Ball Z Movie 01: Ora no Gohan wo Kaese!!</td>\n",
       "      <td>['Piccolo is training at a barren cliff when a...</td>\n",
       "      <td>https://myanimelist.net/anime/2814/Dondon_Domm...</td>\n",
       "      <td>0.046508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6595.0</td>\n",
       "      <td>Nezha Nao Hai</td>\n",
       "      <td>['The film is an adaptation of a story in Chin...</td>\n",
       "      <td>https://myanimelist.net/anime/14693/Yurumates_...</td>\n",
       "      <td>0.044271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11479.0</td>\n",
       "      <td>Kyutai Panic Adventure Returns!</td>\n",
       "      <td>['A special screened at an event named Odaiba ...</td>\n",
       "      <td>https://myanimelist.net/anime/30664/Jinkou_no_...</td>\n",
       "      <td>0.042163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3224.0</td>\n",
       "      <td>Dragon Ball Movie 4: Saikyou e no Michi</td>\n",
       "      <td>[\"A retelling of Dragon Ball's origin with a d...</td>\n",
       "      <td>https://myanimelist.net/anime/2664/Doraemon_Mo...</td>\n",
       "      <td>0.042137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6340.0</td>\n",
       "      <td>Dragon Ball Z: Atsumare! Gokuu World</td>\n",
       "      <td>[\"Dragon Ball Z: Atsumare! Goku's World is a T...</td>\n",
       "      <td>https://myanimelist.net/anime/1153/Crying_Free...</td>\n",
       "      <td>0.038425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5760.0</td>\n",
       "      <td>Dragon Ball Z Movie 04: Super Saiyajin da Son ...</td>\n",
       "      <td>[\"Gohan Son and Piccolo are peacefully playing...</td>\n",
       "      <td>https://myanimelist.net/anime/9549/Alps_no_Sho...</td>\n",
       "      <td>0.036603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Doc                                              Title  \\\n",
       "0   5985.0  Dragon Ball GT: Gokuu Gaiden! Yuuki no Akashi ...   \n",
       "1    364.0                                      Dragon Ball Z   \n",
       "2   6557.0                                        Galo Sengen   \n",
       "3   2267.0  Dragon Ball Z Movie 13: Ryuuken Bakuhatsu!! Go...   \n",
       "4   5275.0    Dragon Ball Z Movie 01: Ora no Gohan wo Kaese!!   \n",
       "5   6595.0                                      Nezha Nao Hai   \n",
       "6  11479.0                    Kyutai Panic Adventure Returns!   \n",
       "7   3224.0            Dragon Ball Movie 4: Saikyou e no Michi   \n",
       "8   6340.0               Dragon Ball Z: Atsumare! Gokuu World   \n",
       "9   5760.0  Dragon Ball Z Movie 04: Super Saiyajin da Son ...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  [\"Years after the end of the Dragonball GT, th...   \n",
       "1  [\"Five years after winning the World Martial A...   \n",
       "2  ['Galo Sengen (\"Galish Man GAL\" in English / G...   \n",
       "3  ['The Z Warriors discover an unopenable music ...   \n",
       "4  ['Piccolo is training at a barren cliff when a...   \n",
       "5  ['The film is an adaptation of a story in Chin...   \n",
       "6  ['A special screened at an event named Odaiba ...   \n",
       "7  [\"A retelling of Dragon Ball's origin with a d...   \n",
       "8  [\"Dragon Ball Z: Atsumare! Goku's World is a T...   \n",
       "9  [\"Gohan Son and Piccolo are peacefully playing...   \n",
       "\n",
       "                                                 URL  Similarity  \n",
       "0  https://myanimelist.net/anime/37422/Cinderella...    0.143356  \n",
       "1  https://myanimelist.net/anime/813/Dragon_Ball_Z\\n    0.136394  \n",
       "2  https://myanimelist.net/anime/40526/Dragon_Ie_...    0.097717  \n",
       "3  https://myanimelist.net/anime/26349/Danna_ga_N...    0.046869  \n",
       "4  https://myanimelist.net/anime/2814/Dondon_Domm...    0.046508  \n",
       "5  https://myanimelist.net/anime/14693/Yurumates_...    0.044271  \n",
       "6  https://myanimelist.net/anime/30664/Jinkou_no_...    0.042163  \n",
       "7  https://myanimelist.net/anime/2664/Doraemon_Mo...    0.042137  \n",
       "8  https://myanimelist.net/anime/1153/Crying_Free...    0.038425  \n",
       "9  https://myanimelist.net/anime/9549/Alps_no_Sho...    0.036603  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7aa32b",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cabccc",
   "metadata": {},
   "source": [
    "In the given problem, the goal is to choose the maximum number of appointments such as the appointments choosen are not consecutive. It means that the personal trainer want needs a break between appointments and so he can't accept two consecutive request. Obviously, all the request are consider in chronological order. The input data is just a list of the requested appointment, and we want to maximize the sum of value (not adiacent) contained in the list.\n",
    "\n",
    "One solution could be the followed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7080a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_sum(arr):\n",
    "    a = 0\n",
    "    b = 0\n",
    "    lst=[]\n",
    "    for i in arr:\n",
    "\n",
    "        if b>a:\n",
    "            c= b\n",
    "        else:\n",
    "            c=a\n",
    "         \n",
    "        a = b + i\n",
    "        b = c\n",
    "     \n",
    "    # return max of a and b\n",
    "    return (b if b>a else a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7aa4c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appointments required: [30, 40, 25, 10, 20, 50, 30, 20] \n",
      "\n",
      "total duration (of the accepted appointment):  125\n"
     ]
    }
   ],
   "source": [
    "app = [30, 40, 25, 10, 20, 50, 30, 20] \n",
    "print('Appointments required:', app, '\\n') \n",
    "\n",
    "duration=find_max_sum(app)\n",
    "\n",
    "print('total duration (of the accepted appointment): ', duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dcfc74",
   "metadata": {},
   "source": [
    "In this algorithm we do not save the element that compose the final sum. For this reason we have to modify this algorithm to store and provide the sub set of the initial array, as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "691c9b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_subs(appointment):\n",
    "    \n",
    "    sums = [0 for j in range(len(appointment))] #appointment for store all sums\n",
    "    \n",
    "    accepted = dict() #we will save in a dictionary all the accepted appointment\n",
    "    \n",
    "    for i in range(len(appointment)):\n",
    "\n",
    "        sums[i] = max(sums[i-1], sums[i-2] + appointment[i]) #choose the sums between the last sum and the updated one with the i-th element\n",
    "        \n",
    "        if max(sums[i-1], sums[i-2] + appointment[i]) == (sums[i-2] + appointment[i]): #store the sum\n",
    "            accepted[sums[i-2], appointment[i]] = sums[i]\n",
    "            \n",
    "    \n",
    "    # we want also to know the element that compose the sums, in the follow list we will store the optimal solution \n",
    "    list_of_success = []\n",
    "    max_value = max(accepted.values())\n",
    "    \n",
    "    #a flag is needed to terminate the while when the sum_values is equal to zero (and so when we finished the l)\n",
    "    flag = list(accepted.keys())[list(accepted.values()).index(max_value)][0]\n",
    "\n",
    "    while flag != 0:\n",
    "        \n",
    "        #having an element of the dictionary as (x, y) : z, we store x in single_values and y in sum_value         \n",
    "        single_values = list(accepted.keys())[list(accepted.values()).index(max_value)][1]  #variable in which we store the single value\n",
    "        sum_value = list(accepted.keys())[list(accepted.values()).index(max_value)][0]      #variable in which we store the cumulative number\n",
    "        \n",
    "        #the single_value is exactly the one choosen by the algorithm, so we store it\n",
    "        list_of_success.append(single_values)\n",
    "        \n",
    "        #update the executing variable\n",
    "        max_value = sum_value\n",
    "        flag = sum_value\n",
    "        \n",
    "    #since we operate from the bottom to the up of the solution, we have to reverse the final array\n",
    "    list_of_success.reverse()\n",
    "    \n",
    "    return sums[-1], list_of_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "193e4ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appointments required: [30, 40, 25, 50, 30, 20] \n",
      "\n",
      "total duration (of the accepted appointment):  110  with the sub set of elements:  [40, 50, 20]\n"
     ]
    }
   ],
   "source": [
    "app = [30, 40, 25, 50, 30, 20] \n",
    "print('Appointments required:', app, '\\n') \n",
    "\n",
    "duration, subs=find_max_subs(app)\n",
    "\n",
    "print('total duration (of the accepted appointment): ', duration, \" with the sub set of elements: \", subs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
