{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64f13d2",
   "metadata": {},
   "source": [
    "# Homework 3 - What is the best anime in the world?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3a15d",
   "metadata": {},
   "source": [
    "## completate i nomi qui <-\n",
    "\n",
    "- Martina Milazzo\n",
    "- Dimitri\n",
    "- Roberta Giorgi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b61cb8",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74c803",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 383/383 [05:05<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#383 pages * 50 animes (19130 total)\n",
    "\n",
    "with open(\"C:/Users/marti/Desktop/HW3/list_anime.txt\", \"w\", encoding='utf-8') as file:\n",
    "    \n",
    "    for page in tqdm(range(0, 383)):\n",
    "    \n",
    "        url = 'https://myanimelist.net/topanime.php?limit=' + str(page*50) #single page URL\n",
    "        response = r.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser') #get html\n",
    "\n",
    "        #extract all the links\n",
    "        for tag in soup.find_all(\"tr\", class_=\"ranking-list\"):\n",
    "            a_list = tag.find_all('a', class_=\"hoverinfo_trigger fl-l ml12 mr8\" ,href=True)\n",
    "            for a in a_list:\n",
    "                link = a['href']\n",
    "                file.write(str(link) + '\\n')\n",
    "                \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e0d6f",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EXECUTE THIS CELL#\n",
    "\n",
    "file = open(\"C:/Users/marti/Desktop/HW3/list_anime.txt\", \"r\", encoding='utf-8')\n",
    "\n",
    "pages_dir=\"C:/Users/marti/Desktop/HW3/Pages\"\n",
    "\n",
    "if not os.path.exists(pages_dir):\n",
    "    os.makedirs(pages_dir)\n",
    "\n",
    "counter_anime = 855\n",
    "page = 18\n",
    "\n",
    "for line in file:\n",
    "\n",
    "    counter_anime += 1\n",
    "    if (counter_anime%50 == 1):\n",
    "        page +=1\n",
    "\n",
    "    request = r.get(line, headers={'Cache-Control': 'no-cache'})\n",
    "    #code=request.status_code\n",
    "    if str(request.status_code)[0] == '4': #CODE\n",
    "        while(True): #while(code[0]!='2'):\n",
    "            time.sleep(120)\n",
    "            request = r.get(line, headers={'Cache-Control': 'no-cache'})\n",
    "            print(request.status_code)\n",
    "            if str(request.status_code)[0] == '2': #questa da eliminare\n",
    "                break       #questa pure\n",
    "            \n",
    "\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "    folder_dir=pages_dir+\"/Page\"+str(page)\n",
    "    if not os.path.exists(folder_dir):\n",
    "        os.makedirs(folder_dir)\n",
    "\n",
    "    txt_dir=folder_dir + \"/article_\" + str(counter_anime)+ \".html\"\n",
    "    \n",
    "    with open(txt_dir, \"w\", encoding='utf-8') as file:\n",
    "        file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "729d9b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19131\n"
     ]
    }
   ],
   "source": [
    "# Count anime links in file\n",
    "\n",
    "file = open(\"C:\\\\Users\\\\marti\\\\Desktop\\\\HW3\\\\list_anime.txt\", \"r\", encoding='utf-8')\n",
    "line_count = 0\n",
    "for line in file:\n",
    "    if line != \"\\n\":\n",
    "        line_count += 1\n",
    "file.close()\n",
    "\n",
    "print(line_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0048d",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac451974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EXECUTE THIS CELL#\n",
    "\n",
    "title = []\n",
    "types = []\n",
    "numEpisode = []\n",
    "release = []\n",
    "end = []\n",
    "numMembers = []\n",
    "score = []\n",
    "users = []\n",
    "rank = []\n",
    "popularity = []\n",
    "description = []\n",
    "related = []\n",
    "characters = []\n",
    "voices = []\n",
    "staff = []\n",
    "\n",
    "pages = os.listdir(\"C:/Users/marti/Desktop/HW3/Pages\")[1:]\n",
    "\n",
    "for p in pages:\n",
    "    n_html = os.listdir(pages+\"/\"+str(p))\n",
    "    for i in range(1, len(n_html)+1):\n",
    "        file = open(pages+\"/\"+str(p)+\"/article_\"+str(i), 'r', encoding=\"utf8\")\n",
    "        anime = BeautifulSoup(file, 'lxml')\n",
    "        \n",
    "        #TITLE\n",
    "        try:\n",
    "            t=anime.strong.contents[0]\n",
    "            title.append(t)\n",
    "        except:\n",
    "            title.append('NA')\n",
    "            \n",
    "\n",
    "        #TYPE\n",
    "        try:\n",
    "            ty=anime.find(text = 'Type:').find_next('a').contents[0]\n",
    "            types.append(ty)\n",
    "        except:\n",
    "            types.append('NA')\n",
    "\n",
    "\n",
    "        #NUM_EPISODE\n",
    "        try:\n",
    "            n=int(anime.find(text = 'Episodes:').next_element.strip())\n",
    "            numEpisode.append(n)\n",
    "        except:\n",
    "            numEpisode.append(n)\n",
    "\n",
    "\n",
    "        #RELEASE_DATE\n",
    "        try:\n",
    "            date = anime.find(text = 'Aired:').next_element.strip()\n",
    "            if len(date)>15:\n",
    "                rl=datetime.strptime(date.split(\" to \")[0], '%b %d, %Y' )\n",
    "            else:\n",
    "                rl=datetime.strptime(date, '%b %d, %Y' )\n",
    "            release.append(rl)\n",
    "        except:\n",
    "            release.append('NA')\n",
    "\n",
    "\n",
    "        #END_DATE\n",
    "        try:\n",
    "            date = anime.find(text = 'Aired:').next_element.strip()\n",
    "            if len(date)>15 and date.split(\" to \")[1] != \"?\":\n",
    "                e=datetime.strptime(date.split(\" to \")[1], '%b %d, %Y' )\n",
    "            else:\n",
    "                e=pd.to_datetime(np.NaN, errors='coerce')\n",
    "            end.append(e)\n",
    "        except:\n",
    "            end.append('NA')\n",
    "\n",
    "\n",
    "        #NUM_MEMBER\n",
    "        try:\n",
    "            num_mem = anime.find(text = 'Members:').next_element\n",
    "            nm=int(num_mem.replace('n', '').replace(',', '').strip())\n",
    "            numMembers.append(nm)\n",
    "        except:\n",
    "            numMembers.append(0)\n",
    "\n",
    "\n",
    "        #SCORE\n",
    "        try:\n",
    "            s=anime.find(text = 'Score:').find_next('span').contents\n",
    "            s=float(s[0])\n",
    "            score.append(s)\n",
    "        except:\n",
    "            score.append(None)\n",
    "\n",
    "\n",
    "        #USERS\n",
    "        try:\n",
    "            us = anime.find(text = 'Score:').find_next('span').find_next('span').contents\n",
    "            u=int(us[0])\n",
    "            users.append(u)\n",
    "        except:\n",
    "            users.append(0)\n",
    "\n",
    "\n",
    "        #RANK\n",
    "        try:\n",
    "            rk = anime.find(text = 'Ranked:').next_element\n",
    "            rk=int(rk.replace('\\n', '').replace('#', '').strip())\n",
    "            rank.append(rk)\n",
    "        except:\n",
    "            rank.append(None)\n",
    "\n",
    "\n",
    "        #POPULARITY\n",
    "        try:\n",
    "            pop = anime.find(text='Popularity:').next_element\n",
    "            pop=int(pop.replace(\"\\n\",\"\").replace('#', '').strip())\n",
    "            popularity.append(pop)\n",
    "        except:\n",
    "            popularity.append(None)\n",
    "\n",
    "\n",
    "        #DESCRIPTION\n",
    "        try:\n",
    "            des=anime.find(text = 'Synopsis:').find_next('p').text\n",
    "            des=des.replace(\"\\n\",\"\")\n",
    "            description.append(des)\n",
    "        except:\n",
    "            description.append('NA')\n",
    "\n",
    "\n",
    "        #RELATED\n",
    "        list = anime.find(text = 'Related Anime')\n",
    "        rel = []\n",
    "\n",
    "        if(list != None):\n",
    "\n",
    "            try:    \n",
    "                list = list.find_next('list')\n",
    "                list = list.find_all('a')\n",
    "\n",
    "                for t in list:\n",
    "                    rel.append(t.text)\n",
    "\n",
    "            except:\n",
    "                for t in list:\n",
    "                    rel.append('NA')\n",
    "\n",
    "        related.append(rel)\n",
    "\n",
    "        #CHARACTER\n",
    "        c = []\n",
    "        try:\n",
    "            list = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "            list = list.find_all('list')\n",
    "\n",
    "            for t in list:\n",
    "                people = t.find_all('h3')\n",
    "                for p in people:\n",
    "                    c.append(p.text)\n",
    "        except:\n",
    "            c.append('NA')\n",
    "            \n",
    "        characters.append(c)\n",
    "\n",
    "\n",
    "        #VOICES\n",
    "        v=[]\n",
    "        try:\n",
    "            list = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "            list = list.find_all('h3')\n",
    "\n",
    "            for t in list:\n",
    "                people = el.find_next('list')\n",
    "                for person in people:\n",
    "                    v.append(person.find('a').text)\n",
    "        except:\n",
    "            v.append('NA')\n",
    "\n",
    "        voices.append(v)\n",
    "\n",
    "        #STAFF\n",
    "        st = []\n",
    "        try:\n",
    "            list = anime.find_all(text = 'Staff')[1].find_next(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "            \n",
    "            if(list != None):    \n",
    "                table = list.find_all(\"table\")\n",
    "                for t in table:\n",
    "                    i = t.find_all(\"td\")[1]\n",
    "                    p = [i.find(\"a\").text, i.find(\"small\").text]\n",
    "                    st.append(p)\n",
    "        except:\n",
    "            st.append(p)\n",
    "\n",
    "        staff.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b45399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EXECUTE THIS CELL#\n",
    "\n",
    "col = ['animeTitle', 'animeType', 'animeNumEpisode','releaseDate', 'endDate', 'animeNumMembers', 'animeScore', 'animeUsers', \n",
    "       'animeRank', 'animePopularity', 'animdeDescription', 'animeRelated', 'animeCharacters', 'animeVoices', 'animeStaff']\n",
    "\n",
    "types = {'animeTitle' : 'object', \n",
    "         'animeType' : 'object', \n",
    "         'animeNumEpisode' : 'int64',\n",
    "         'releaseDate' : 'datetime64', \n",
    "         'endDate' : 'datetime64', \n",
    "         'animeNumMembers' : 'int64', \n",
    "         'animeScore' : 'float64',\n",
    "         'animeUsers' : 'int64', \n",
    "         'animeRank' : 'int64',\n",
    "         'animePopularity' : 'int64',\n",
    "         'animdeDescription' : 'object',\n",
    "         'animeRelated' : 'object',\n",
    "         'animeCharacters' : 'object',\n",
    "         'animeVoices' : 'object',\n",
    "         'animeStaff' : 'object'}\n",
    "\n",
    "data = list(zip(title,types,numEpisode,release,end,numMembers,score,users,rank,popularity,description,related,characters,voices,staff))\n",
    "\n",
    "dataset = pd.DataFrame(data, columns = col).astype(dtype = types)  \n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    with open('C:/Users/marti/Desktop/HW3/tsv/anime_'+str(i)+'.tsv', 'w') as file:\n",
    "        tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "        tsv_writer.writerow([x for x in dataset.columns]) \n",
    "        tsv_writer.writerow(x for x in dataset.iloc[i]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac4605-7169-4017-9fca-a4be171fee2d",
   "metadata": {},
   "source": [
    "# 2. Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae83e1c0-baca-4144-a926-8d996e42ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Get a list of the animes\n",
    "animes = os.listdir('/home/roberta/dataScience/01/Algorithmic Methods of Data Mining and Laboratory/hw3/tsv_files_full')\n",
    "\n",
    "# Save column names\n",
    "names = ['Title', 'Type', 'Episodes', 'Members', 'Score', 'Users', 'Rank', 'Popularity', 'Description', 'Related', 'Characters', 'Voices', 'Staff', 'release', 'end']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1f769-c58b-4c10-9dc5-c91ea53e380c",
   "metadata": {},
   "source": [
    "### 2.0.1 Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e802475-ee90-4ec9-999c-5554c1cf5876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/roberta/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "298db704-bd30-4f01-b69f-551a62a12bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(anime):\n",
    "    \n",
    "    # Save the English stopwords in a variable\n",
    "    en_stops = set(stopwords.words('english'))\n",
    "    \n",
    "    for column in anime:\n",
    "        processed_prop = ''\n",
    "        for prop in anime[column]:\n",
    "            if type(prop) is str:\n",
    "                for word in prop.split():\n",
    "                    if word not in en_stops:\n",
    "                        processed_prop += word + ' '\n",
    "                processed_df.at[0, column] = processed_prop\n",
    "            else:\n",
    "                processed_df.at[0, column] = prop\n",
    "                \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d10f13-7ada-44c0-b385-d5ef8802e36f",
   "metadata": {},
   "source": [
    "### 2.0.2 Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c662063-b5c1-4022-a61f-424f4813e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(anime):\n",
    "    \n",
    "    for column in anime:\n",
    "        for prop in anime[column]:\n",
    "            words = nltk.word_tokenize(str(prop))\n",
    "            processed = [word for word in words if word.isalnum()]\n",
    "            processed_df.at[0, column] = ' '.join(processed)\n",
    "            \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e765e8-65af-480c-800d-0452243204f4",
   "metadata": {},
   "source": [
    "### 2.0.3 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ddfb9b7-501b-499e-940e-23f520a41df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(anime):\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for column in anime:\n",
    "        processed_prop = ''\n",
    "        for prop in anime[column]:\n",
    "            if type(prop) is str:\n",
    "                for word in prop.split():\n",
    "                    processed_prop += ps.stem(word) + ' '\n",
    "                processed_df.at[0, column] = processed_prop\n",
    "            else:\n",
    "                processed_df.at[0, column] = prop\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e47e0872-f8ba-4945-a199-bf2efc3b0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anime_i in range(len(animes)):\n",
    "    \n",
    "    anime = pd.read_csv('/home/roberta/dataScience/01/Algorithmic Methods of Data Mining and Laboratory/hw3/tsv_files_full/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "\n",
    "    processed_df = pd.DataFrame(columns=names)    \n",
    "    processed_df = removeStopwords(anime)\n",
    "    processed_df = removePunctuation(processed_df)\n",
    "    processed_df = stemming(processed_df)\n",
    "\n",
    "    processed_df.to_csv('/home/roberta/dataScience/01/Algorithmic Methods of Data Mining and Laboratory/hw3/tsv_processed/anime_{i}.tsv'.format(i=anime_i), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980fa2b-3543-49aa-a1c1-5ed66bef7b5e",
   "metadata": {},
   "source": [
    "### 2.1.1 Create the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c1eab05-9cf5-489a-af81-2223f41bb37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_lst = []\n",
    "for anime_i in range(len(animes)):\n",
    "    anime = pd.read_csv('/home/roberta/dataScience/01/Algorithmic Methods of Data Mining and Laboratory/hw3/tsv_processed/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "    animes_lst.append(anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dcab93ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Title Type  Episodes  Members Score    Users  \\\n",
      "0    fullmet alchemist brotherhood   tv         64  2675751   NaN  1622384   \n",
      "1                               NaN  tv         51   483807   NaN   169476   \n",
      "2  shingeki kyojin season 3 part 2   tv         10  1596039   NaN  1087519   \n",
      "3                       stein gate   tv         24  2090910   NaN  1109700   \n",
      "4           fruit basket the final   tv         13   275214   NaN   113310   \n",
      "\n",
      "  Rank  Popularity                                        Description  \\\n",
      "0    1           3  after horrif alchemi experi goe wrong elric ho...   \n",
      "1    2         337  gintoki shinpachi kagura return broke member y...   \n",
      "2    3          33  restor diminish hope survey corp embark missio...   \n",
      "3    4          11  mad scientist rintar okab rent room ricketi ol...   \n",
      "4    5         651  year ago chines zodiac spirit god swore stay t...   \n",
      "\n",
      "                                             Related  \\\n",
      "0  fullmet alchemist version fullmet alchemist st...   \n",
      "1  gintama gintama movi 2 yorozuya yo eien nare s...   \n",
      "2  shingeki kyojin shingeki kyojin season 3 shing...   \n",
      "3  stein gate set chäo head stein gate oukoubakko...   \n",
      "4  fruit basket fruit basket 2nd season fruit bas...   \n",
      "\n",
      "                                          Characters  \\\n",
      "0  edward alphons roy mae riza ling alex loui win...   \n",
      "1  gintoki shinpachi kotar toushir sougo shinsuk ...   \n",
      "2  eren mikasa armin erwin hang sasha reiner jean...   \n",
      "3  rintar kurisu mayuri itaru suzuha ruka rumiho ...   \n",
      "4  kyou tooru yuki hatsuharu momiji shigur ayam h...   \n",
      "\n",
      "                                              Voices  \\\n",
      "0  romi rie shinichiro keiji yuuichi fumiko mamor...   \n",
      "1  tomokazu rie daisuk akira kazuya kenichi takeh...   \n",
      "2  hiroshi yuki yui marina daisuk romi yuu yoshim...   \n",
      "3  mamoru asami kana tomokazu yukari yuu haruko s...   \n",
      "4  yuuma manaka nobunaga makoto megumi yuuichi ta...   \n",
      "\n",
      "                                               Staff release   end  \n",
      "0  justin noritomo yasuhiro episod director story...    nan   nan   \n",
      "1  youichi storyboard plan chizuru storyboard key...    nan   nan   \n",
      "2                shuuhei jouji katsuji plan tetsuya     nan   nan   \n",
      "3  gaku takeshi plan hiroshi episod director stor...    nan   nan   \n",
      "4  yoshihid jin director up song perform song per...    nan   nan   \n"
     ]
    }
   ],
   "source": [
    "#print(animes_lst)    \n",
    "\n",
    "animes_df = pd.concat(animes_lst)\n",
    "animes_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "animes_df.reset_index(inplace = True, drop=True)\n",
    "print(animes_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6e5db8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "vocabulary = {}\n",
    "inverted_index = collections.defaultdict(list)\n",
    "term_id = 1\n",
    "\n",
    "for index, anime in animes_df.iterrows():\n",
    "\n",
    "    description = anime['Description']  # 'Ciao mi chiamo Roberta'\n",
    "    if type(description) is str:\n",
    "        words = description.split()  # ['ciao', 'mi', 'chiamo', 'Roberta']\n",
    "\n",
    "        for word in words:\n",
    "\n",
    "            # Create vocabulary\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = term_id\n",
    "                term_id += 1\n",
    "\n",
    "            # Create inverted index\n",
    "            if index not in inverted_index[term_id]:\n",
    "                inverted_index[term_id].append(index)\n",
    "\n",
    "#inverted_index.to_json('/home/roberta/dataScience/01/Algorithmic Methods of Data Mining and Laboratory/hw3/tsv_dataset/inverted_index.json', sep='\\t', header=False, na_rep='\\t')\n",
    "#with open('/home/roberta/dataScience/01/Algorithmic Methods of Data Mining and Laboratory/hw3/inverted_index_0.csv', \"wt\") as f:\n",
    "#    w = csv.DictWriter(f, inverted_index.keys())\n",
    "#    w.writeheader()\n",
    "#    w.writerow(inverted_index)\n",
    "\n",
    "with open('/home/roberta/dataScience/01/Algorithmic Methods of Data Mining and Laboratory/hw3/inverted_index_2.csv', \"wt\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in inverted_index.items():\n",
    "       writer.writerow([key, value])\n",
    "\n",
    "#with open('/home/roberta/dataScience/01/Algorithmic Methods of Data Mining and Laboratory/hw3/inverted_index_0.csv',\n",
    "#          \"rt\") as f:\n",
    "#    for i in range(1):\n",
    "#        line = f.readline()\n",
    "#        print(line)\n",
    "\n",
    "with open('/home/roberta/dataScience/01/Algorithmic Methods of Data Mining and Laboratory/hw3/inverted_index_2.csv') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    mydict = dict(reader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
