{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64f13d2",
   "metadata": {},
   "source": [
    "# Homework 3 - What is the best anime in the world?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3a15d",
   "metadata": {},
   "source": [
    "- Martina Milazzo\n",
    "- Dimitri Saliola\n",
    "- Roberta Giorgi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b61cb8",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74c803",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 383/383 [05:05<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#383 pages * 50 animes (19130 total)\n",
    "\n",
    "with open(\"C:/Users/marti/Desktop/HW3/list_anime.txt\", \"w\", encoding='utf-8') as file:\n",
    "    \n",
    "    for page in tqdm(range(0, 383)):\n",
    "    \n",
    "        url = 'https://myanimelist.net/topanime.php?limit=' + str(page*50) #single page URL\n",
    "        response = r.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser') #get html\n",
    "\n",
    "        #extract all the links\n",
    "        for tag in soup.find_all(\"tr\", class_=\"ranking-list\"):\n",
    "            a_list = tag.find_all('a', class_=\"hoverinfo_trigger fl-l ml12 mr8\" ,href=True)\n",
    "            for a in a_list:\n",
    "                link = a['href']\n",
    "                file.write(str(link) + '\\n')\n",
    "                \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e0d6f",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EXECUTE THIS CELL#\n",
    "\n",
    "file = open(\"C:/Users/marti/Desktop/HW3/list_anime.txt\", \"r\", encoding='utf-8')\n",
    "\n",
    "pages_dir=\"C:/Users/marti/Desktop/HW3/Pages\"\n",
    "\n",
    "if not os.path.exists(pages_dir):\n",
    "    os.makedirs(pages_dir)\n",
    "\n",
    "counter_anime = 855\n",
    "page = 18\n",
    "\n",
    "for line in file:\n",
    "\n",
    "    counter_anime += 1\n",
    "    if (counter_anime%50 == 1):\n",
    "        page +=1\n",
    "\n",
    "    request = r.get(line, headers={'Cache-Control': 'no-cache'})\n",
    "    #code=request.status_code\n",
    "    if str(request.status_code)[0] == '4': #CODE\n",
    "        while(True): #while(code[0]!='2'):\n",
    "            time.sleep(120)\n",
    "            request = r.get(line, headers={'Cache-Control': 'no-cache'})\n",
    "            print(request.status_code)\n",
    "            if str(request.status_code)[0] == '2': #questa da eliminare\n",
    "                break       #questa pure\n",
    "            \n",
    "\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "    folder_dir=pages_dir+\"/Page\"+str(page)\n",
    "    if not os.path.exists(folder_dir):\n",
    "        os.makedirs(folder_dir)\n",
    "\n",
    "    txt_dir=folder_dir + \"/article_\" + str(counter_anime)+ \".html\"\n",
    "    \n",
    "    with open(txt_dir, \"w\", encoding='utf-8') as file:\n",
    "        file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "729d9b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19131\n"
     ]
    }
   ],
   "source": [
    "# Count anime links in file\n",
    "\n",
    "file = open(\"C:\\\\Users\\\\marti\\\\Desktop\\\\HW3\\\\list_anime.txt\", \"r\", encoding='utf-8')\n",
    "line_count = 0\n",
    "for line in file:\n",
    "    if line != \"\\n\":\n",
    "        line_count += 1\n",
    "file.close()\n",
    "\n",
    "print(line_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0048d",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac451974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EXECUTE THIS CELL#\n",
    "\n",
    "title = []\n",
    "types = []\n",
    "numEpisode = []\n",
    "release = []\n",
    "end = []\n",
    "numMembers = []\n",
    "score = []\n",
    "users = []\n",
    "rank = []\n",
    "popularity = []\n",
    "description = []\n",
    "related = []\n",
    "characters = []\n",
    "voices = []\n",
    "staff = []\n",
    "\n",
    "pages = os.listdir(\"C:/Users/marti/Desktop/HW3/Pages\")[1:]\n",
    "\n",
    "for p in pages:\n",
    "    n_html = os.listdir(pages+\"/\"+str(p))\n",
    "    for i in range(1, len(n_html)+1):\n",
    "        file = open(pages+\"/\"+str(p)+\"/article_\"+str(i), 'r', encoding=\"utf8\")\n",
    "        anime = BeautifulSoup(file, 'lxml')\n",
    "        \n",
    "        #TITLE\n",
    "        try:\n",
    "            t=anime.strong.contents[0]\n",
    "            title.append(t)\n",
    "        except:\n",
    "            title.append('NA')\n",
    "            \n",
    "\n",
    "        #TYPE\n",
    "        try:\n",
    "            ty=anime.find(text = 'Type:').find_next('a').contents[0]\n",
    "            types.append(ty)\n",
    "        except:\n",
    "            types.append('NA')\n",
    "\n",
    "\n",
    "        #NUM_EPISODE\n",
    "        try:\n",
    "            n=int(anime.find(text = 'Episodes:').next_element.strip())\n",
    "            numEpisode.append(n)\n",
    "        except:\n",
    "            numEpisode.append(n)\n",
    "\n",
    "\n",
    "        #RELEASE_DATE\n",
    "        try:\n",
    "            date = anime.find(text = 'Aired:').next_element.strip()\n",
    "            if len(date)>15:\n",
    "                rl=datetime.strptime(date.split(\" to \")[0], '%b %d, %Y' )\n",
    "            else:\n",
    "                rl=datetime.strptime(date, '%b %d, %Y' )\n",
    "            release.append(rl)\n",
    "        except:\n",
    "            release.append('NA')\n",
    "\n",
    "\n",
    "        #END_DATE\n",
    "        try:\n",
    "            date = anime.find(text = 'Aired:').next_element.strip()\n",
    "            if len(date)>15 and date.split(\" to \")[1] != \"?\":\n",
    "                e=datetime.strptime(date.split(\" to \")[1], '%b %d, %Y' )\n",
    "            else:\n",
    "                e=pd.to_datetime(np.NaN, errors='coerce')\n",
    "            end.append(e)\n",
    "        except:\n",
    "            end.append('NA')\n",
    "\n",
    "\n",
    "        #NUM_MEMBER\n",
    "        try:\n",
    "            num_mem = anime.find(text = 'Members:').next_element\n",
    "            nm=int(num_mem.replace('n', '').replace(',', '').strip())\n",
    "            numMembers.append(nm)\n",
    "        except:\n",
    "            numMembers.append(0)\n",
    "\n",
    "\n",
    "        #SCORE\n",
    "        try:\n",
    "            s=anime.find(text = 'Score:').find_next('span').contents\n",
    "            s=float(s[0])\n",
    "            score.append(s)\n",
    "        except:\n",
    "            score.append(None)\n",
    "\n",
    "\n",
    "        #USERS\n",
    "        try:\n",
    "            us = anime.find(text = 'Score:').find_next('span').find_next('span').contents\n",
    "            u=int(us[0])\n",
    "            users.append(u)\n",
    "        except:\n",
    "            users.append(0)\n",
    "\n",
    "\n",
    "        #RANK\n",
    "        try:\n",
    "            rk = anime.find(text = 'Ranked:').next_element\n",
    "            rk=int(rk.replace('\\n', '').replace('#', '').strip())\n",
    "            rank.append(rk)\n",
    "        except:\n",
    "            rank.append(None)\n",
    "\n",
    "\n",
    "        #POPULARITY\n",
    "        try:\n",
    "            pop = anime.find(text='Popularity:').next_element\n",
    "            pop=int(pop.replace(\"\\n\",\"\").replace('#', '').strip())\n",
    "            popularity.append(pop)\n",
    "        except:\n",
    "            popularity.append(None)\n",
    "\n",
    "\n",
    "        #DESCRIPTION\n",
    "        try:\n",
    "            des=anime.find(text = 'Synopsis:').find_next('p').text\n",
    "            des=des.replace(\"\\n\",\"\")\n",
    "            description.append(des)\n",
    "        except:\n",
    "            description.append('NA')\n",
    "\n",
    "\n",
    "        #RELATED\n",
    "        list = anime.find(text = 'Related Anime')\n",
    "        rel = []\n",
    "\n",
    "        if(list != None):\n",
    "\n",
    "            try:    \n",
    "                list = list.find_next('list')\n",
    "                list = list.find_all('a')\n",
    "\n",
    "                for t in list:\n",
    "                    rel.append(t.text)\n",
    "\n",
    "            except:\n",
    "                for t in list:\n",
    "                    rel.append('NA')\n",
    "\n",
    "        related.append(rel)\n",
    "\n",
    "        #CHARACTER\n",
    "        c = []\n",
    "        try:\n",
    "            list = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "            list = list.find_all('list')\n",
    "\n",
    "            for t in list:\n",
    "                people = t.find_all('h3')\n",
    "                for p in people:\n",
    "                    c.append(p.text)\n",
    "        except:\n",
    "            c.append('NA')\n",
    "            \n",
    "        characters.append(c)\n",
    "\n",
    "\n",
    "        #VOICES\n",
    "        v=[]\n",
    "        try:\n",
    "            list = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "            list = list.find_all('h3')\n",
    "\n",
    "            for t in list:\n",
    "                people = el.find_next('list')\n",
    "                for person in people:\n",
    "                    v.append(person.find('a').text)\n",
    "        except:\n",
    "            v.append('NA')\n",
    "\n",
    "        voices.append(v)\n",
    "\n",
    "        #STAFF\n",
    "        st = []\n",
    "        try:\n",
    "            list = anime.find_all(text = 'Staff')[1].find_next(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "            \n",
    "            if(list != None):    \n",
    "                table = list.find_all(\"table\")\n",
    "                for t in table:\n",
    "                    i = t.find_all(\"td\")[1]\n",
    "                    p = [i.find(\"a\").text, i.find(\"small\").text]\n",
    "                    st.append(p)\n",
    "        except:\n",
    "            st.append(p)\n",
    "\n",
    "        staff.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b45399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EXECUTE THIS CELL#\n",
    "\n",
    "col = ['animeTitle', 'animeType', 'animeNumEpisode','releaseDate', 'endDate', 'animeNumMembers', 'animeScore', 'animeUsers', \n",
    "       'animeRank', 'animePopularity', 'animdeDescription', 'animeRelated', 'animeCharacters', 'animeVoices', 'animeStaff']\n",
    "\n",
    "types = {'animeTitle' : 'object', \n",
    "         'animeType' : 'object', \n",
    "         'animeNumEpisode' : 'int64',\n",
    "         'releaseDate' : 'datetime64', \n",
    "         'endDate' : 'datetime64', \n",
    "         'animeNumMembers' : 'int64', \n",
    "         'animeScore' : 'float64',\n",
    "         'animeUsers' : 'int64', \n",
    "         'animeRank' : 'int64',\n",
    "         'animePopularity' : 'int64',\n",
    "         'animdeDescription' : 'object',\n",
    "         'animeRelated' : 'object',\n",
    "         'animeCharacters' : 'object',\n",
    "         'animeVoices' : 'object',\n",
    "         'animeStaff' : 'object'}\n",
    "\n",
    "data = list(zip(title,types,numEpisode,release,end,numMembers,score,users,rank,popularity,description,related,characters,voices,staff))\n",
    "\n",
    "dataset = pd.DataFrame(data, columns = col).astype(dtype = types)  \n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    with open('C:/Users/marti/Desktop/HW3/tsv/anime_'+str(i)+'.tsv', 'w') as file:\n",
    "        tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "        tsv_writer.writerow([x for x in dataset.columns]) \n",
    "        tsv_writer.writerow(x for x in dataset.iloc[i]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac4605-7169-4017-9fca-a4be171fee2d",
   "metadata": {},
   "source": [
    "# 2. Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae83e1c0-baca-4144-a926-8d996e42ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "\n",
    "# Get a list of the animes\n",
    "animes = os.listdir('C:/Users/marti/Desktop/tsv_files_full')\n",
    "\n",
    "# Save column names\n",
    "names = ['Title', 'Type', 'Episodes', 'Members', 'Score', 'Users', 'Rank', 'Popularity', 'Description', 'Related', 'Characters', 'Voices', 'Staff', 'release', 'end']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1f769-c58b-4c10-9dc5-c91ea53e380c",
   "metadata": {},
   "source": [
    "### 2.0.1 Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e802475-ee90-4ec9-999c-5554c1cf5876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298db704-bd30-4f01-b69f-551a62a12bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(anime):\n",
    "    \n",
    "    # Save the English stopwords in a variable\n",
    "    en_stops = set(stopwords.words('english'))\n",
    "    \n",
    "    for column in anime:\n",
    "        processed_prop = ''\n",
    "        for prop in anime[column]:\n",
    "            if type(prop) is str:\n",
    "                for word in prop.split():\n",
    "                    if word not in en_stops:\n",
    "                        processed_prop += word + ' '\n",
    "                processed_df.at[0, column] = processed_prop\n",
    "            else:\n",
    "                processed_df.at[0, column] = prop\n",
    "                \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d10f13-7ada-44c0-b385-d5ef8802e36f",
   "metadata": {},
   "source": [
    "### 2.0.2 Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9ff906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c662063-b5c1-4022-a61f-424f4813e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(anime):\n",
    "    \n",
    "    for column in anime:\n",
    "        for prop in anime[column]:\n",
    "            words = nltk.word_tokenize(str(prop))\n",
    "            processed = [word for word in words if word.isalnum()]\n",
    "            processed_df.at[0, column] = ' '.join(processed)\n",
    "            \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e765e8-65af-480c-800d-0452243204f4",
   "metadata": {},
   "source": [
    "### 2.0.3 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ddfb9b7-501b-499e-940e-23f520a41df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(anime):\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for column in anime:\n",
    "        processed_prop = ''\n",
    "        for prop in anime[column]:\n",
    "            if type(prop) is str:\n",
    "                for word in prop.split():\n",
    "                    processed_prop += ps.stem(word) + ' '\n",
    "                processed_df.at[0, column] = processed_prop\n",
    "            else:\n",
    "                processed_df.at[0, column] = prop\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e47e0872-f8ba-4945-a199-bf2efc3b0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anime_i in range(len(animes)):\n",
    "    \n",
    "    anime = pd.read_csv('C:/Users/marti/Desktop/tsv_files_full/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "\n",
    "    processed_df = pd.DataFrame(columns=names)    \n",
    "    processed_df = removeStopwords(anime)\n",
    "    processed_df = removePunctuation(processed_df)\n",
    "    processed_df = stemming(processed_df)\n",
    "\n",
    "    processed_df.to_csv('C:/Users/marti/Desktop/tsv_processed/anime_{i}.tsv'.format(i=anime_i), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980fa2b-3543-49aa-a1c1-5ed66bef7b5e",
   "metadata": {},
   "source": [
    "### 2.1.1 Create the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c1eab05-9cf5-489a-af81-2223f41bb37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_lst = []\n",
    "for anime_i in range(len(animes)):\n",
    "    anime = pd.read_csv('C:/Users/marti/Desktop/tsv_processed/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "    animes_lst.append(anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcab93ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Title Type  Episodes  Members Score    Users  \\\n",
      "0    fullmet alchemist brotherhood   tv         64  2675751   NaN  1622384   \n",
      "1                               NaN  tv         51   483807   NaN   169476   \n",
      "2  shingeki kyojin season 3 part 2   tv         10  1596039   NaN  1087519   \n",
      "3                       stein gate   tv         24  2090910   NaN  1109700   \n",
      "4           fruit basket the final   tv         13   275214   NaN   113310   \n",
      "\n",
      "  Rank  Popularity                                        Description  \\\n",
      "0    1           3  after horrif alchemi experi goe wrong elric ho...   \n",
      "1    2         337  gintoki shinpachi kagura return broke member y...   \n",
      "2    3          33  restor diminish hope survey corp embark missio...   \n",
      "3    4          11  mad scientist rintar okab rent room ricketi ol...   \n",
      "4    5         651  year ago chines zodiac spirit god swore stay t...   \n",
      "\n",
      "                                             Related  \\\n",
      "0  fullmet alchemist version fullmet alchemist st...   \n",
      "1  gintama gintama movi 2 yorozuya yo eien nare s...   \n",
      "2  shingeki kyojin shingeki kyojin season 3 shing...   \n",
      "3  stein gate set chäo head stein gate oukoubakko...   \n",
      "4  fruit basket fruit basket 2nd season fruit bas...   \n",
      "\n",
      "                                          Characters  \\\n",
      "0  edward alphons roy mae riza ling alex loui win...   \n",
      "1  gintoki shinpachi kotar toushir sougo shinsuk ...   \n",
      "2  eren mikasa armin erwin hang sasha reiner jean...   \n",
      "3  rintar kurisu mayuri itaru suzuha ruka rumiho ...   \n",
      "4  kyou tooru yuki hatsuharu momiji shigur ayam h...   \n",
      "\n",
      "                                              Voices  \\\n",
      "0  romi rie shinichiro keiji yuuichi fumiko mamor...   \n",
      "1  tomokazu rie daisuk akira kazuya kenichi takeh...   \n",
      "2  hiroshi yuki yui marina daisuk romi yuu yoshim...   \n",
      "3  mamoru asami kana tomokazu yukari yuu haruko s...   \n",
      "4  yuuma manaka nobunaga makoto megumi yuuichi ta...   \n",
      "\n",
      "                                               Staff release   end  \n",
      "0  justin noritomo yasuhiro episod director story...    nan   nan   \n",
      "1  youichi storyboard plan chizuru storyboard key...    nan   nan   \n",
      "2                shuuhei jouji katsuji plan tetsuya     nan   nan   \n",
      "3  gaku takeshi plan hiroshi episod director stor...    nan   nan   \n",
      "4  yoshihid jin director up song perform song per...    nan   nan   \n"
     ]
    }
   ],
   "source": [
    "animes_df = pd.concat(animes_lst)\n",
    "animes_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "animes_df.reset_index(inplace = True, drop=True)\n",
    "print(animes_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e5db8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(dataframe):\n",
    "    vocabulary = {}\n",
    "    term_id = 1\n",
    "\n",
    "    for index, anime in dataframe.iterrows():\n",
    "\n",
    "        description = anime['Description']  # 'Ciao mi chiamo Roberta'\n",
    "        if type(description) is str:\n",
    "            words = description.split()  # ['ciao', 'mi', 'chiamo', 'Roberta']\n",
    "\n",
    "            for word in words:\n",
    "\n",
    "                # Create vocabulary\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = term_id\n",
    "                    term_id += 1\n",
    "          \n",
    "    return vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83d9a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(vocabulary, dataframe):\n",
    "    inverted_index={}\n",
    "    for index, anime in dataframe.iterrows():\n",
    "\n",
    "        description = anime['Description']  # 'Ciao mi chiamo Roberta'\n",
    "        if type(description) is str:\n",
    "            words = description.split()  # ['ciao', 'mi', 'chiamo', 'Roberta']\n",
    "\n",
    "            for word in words:\n",
    "                term_id = vocabulary[word]\n",
    "                if term_id not in inverted_index:\n",
    "                    inverted_index[term_id] = []\n",
    "                if index not in inverted_index[term_id]:\n",
    "                    inverted_index[term_id].append(index)\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35905078",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=create_vocabulary(animes_df)\n",
    "inverted_index=create_inverted_index(vocabulary, animes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f2db0",
   "metadata": {},
   "source": [
    "## 2.1.2 execute query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb53306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary to return the original description of the anime (not the processed one)\n",
    "animes_original = []\n",
    "for anime_i in range(len(animes)):\n",
    "    anime_original = pd.read_csv('C:/Users/marti/Desktop/tsv_files_full/anime_{i}.tsv'.format(i=anime_i), sep='\\t')\n",
    "    animes_original.append(anime_original)\n",
    "animes_original = pd.concat(animes_original)\n",
    "animes_original = animes_original.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85d92a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "animes_original.to_csv('C:/Users/marti/Desktop/file_originali_tsv/anime_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60405d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary to get the urls for each anime\n",
    "with open('C:/Users/marti/Desktop/HW3/list_anime.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e398f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query):\n",
    "    #query=\"after\"\n",
    "    term_id=0\n",
    "    first=True\n",
    "    query=query.split()\n",
    "    q_match=pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "\n",
    "    try:\n",
    "        for term in query:\n",
    "            if term in vocabulary:\n",
    "                term_id=vocabulary[term]\n",
    "            if first:\n",
    "                lst_doc=inverted_index[term_id]\n",
    "                first=False\n",
    "            else:\n",
    "                lst_doc=set(lst_doc).intersection(inverted_index[term_id])\n",
    "                if len(lst_doc)==0:\n",
    "                    print('no doc found')\n",
    "                    break\n",
    "\n",
    "        for i, doc in enumerate(lst_doc):\n",
    "            q_match.at[i, ['Title', 'Description', 'URL']]=animes_original.iloc[doc]['Title'], animes_original.iloc[doc]['Description'], lines[doc]\n",
    "            #q_match.at[i, 'Description']=animes_original.iloc[doc]['Description']\n",
    "            #q_match.at[i, 'URL']=lines[doc]\n",
    "        \n",
    "        display(q_match)\n",
    "    except:\n",
    "        print(\"no doc found\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f29dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_query(query):\n",
    "    tokenized_query = nltk.word_tokenize(query)\n",
    "    en_stops = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    # Process the query before executing it\n",
    "    processed_final=\" \"\n",
    "\n",
    "    remove_punctuation = [t for t in tokenized_query if t.isalnum()]\n",
    "    #print(\"1: \", remove_punctuation)\n",
    "    remove_stopwords = [t for t in remove_punctuation if t not in en_stops]\n",
    "    #print(\"2: \", remove_stopwords)\n",
    "    \n",
    "    for word in remove_stopwords:\n",
    "        processed_final +=ps.stem(word)+\" \"\n",
    "\n",
    "    #print(\"3: \", processed_final)\n",
    "    \n",
    "    return processed_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea4663af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " home live hous comput text keyboard phone \n"
     ]
    }
   ],
   "source": [
    "query=\"home lives house computing texting keyboard phone\"\n",
    "proc=pre_process_query(query)\n",
    "print(proc)\n",
    "#execute_query(processed_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf60158",
   "metadata": {},
   "source": [
    "## 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f58ae",
   "metadata": {},
   "source": [
    "### 2.2.1) inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b590932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_second_inverted_index(vocabulary, inverted_index, dataframe):\n",
    "\n",
    "    second_inverted_index={}\n",
    "    \n",
    "    for index, anime in dataframe.iterrows():\n",
    "        vocabulary_occurrences = {}\n",
    "        occurrence=[]\n",
    "\n",
    "        description = anime['Description']  # 'Ciao mi chiamo Roberta'\n",
    "        if type(description) is str:\n",
    "            words = description.split()  # ['ciao', 'mi', 'chiamo', 'Roberta']\n",
    "\n",
    "            for word in words:\n",
    "                if word not in occurrence:\n",
    "                    vocabulary_occurrences[word]=words.count(word)\n",
    "\n",
    "\n",
    "            tot_words_in_doc=len(words)\n",
    "\n",
    "            for word in vocabulary_occurrences:\n",
    "                term_id=vocabulary[word]\n",
    "\n",
    "                #term frequency=#word appears in doc / #tot words in doc\n",
    "                word_occur=vocabulary_occurrences[word]\n",
    "                tf=word_occur/tot_words_in_doc\n",
    "\n",
    "                #inverse data frequency= log(#tot_docs/#docs_containing_term_i)\n",
    "                tot_docs=len(dataframe)\n",
    "                docs_with_word=len(inverted_index[term_id])\n",
    "                idf=math.log(tot_docs/docs_with_word, 10)\n",
    "\n",
    "                #tf-idf\n",
    "                tf_idf=tf*idf\n",
    "\n",
    "                if term_id not in second_inverted_index:\n",
    "                    second_inverted_index[term_id] = []\n",
    "                if index not in second_inverted_index[term_id]:\n",
    "                    second_inverted_index[term_id].append((index, tf_idf))\n",
    "\n",
    "    return second_inverted_index\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95a7f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#animes_df = pd.read_csv('/path') #da ricontrollare\n",
    "vocabulary = create_vocabulary(animes_df)\n",
    "inverted_index = create_inverted_index(vocabulary, animes_df)\n",
    "\n",
    "second_inverted_index=create_second_inverted_index(vocabulary, inverted_index, animes_df)\n",
    "\n",
    "#print(second_inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8302be",
   "metadata": {},
   "source": [
    "### 2.2.2) Execute query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0448d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents_to_word(dataframe, doc, vocabulary):\n",
    "    vocabulary_occurrences={}\n",
    "    occurrence=[]\n",
    "    description=dataframe.iloc[doc]['Description']\n",
    "    norm1=0\n",
    "    description=description.split()\n",
    "\n",
    "    \n",
    "    for word in description:\n",
    "\n",
    "        if word not in occurrence:\n",
    "            vocabulary_occurrences[word]=description.count(word)\n",
    "\n",
    "\n",
    "\n",
    "    tot_words_in_doc=len(description)\n",
    "\n",
    "    for word in vocabulary_occurrences:\n",
    "        term_id=vocabulary[word]\n",
    "\n",
    "        #term frequency=#word appears in doc / #tot words in doc\n",
    "        word_occur=vocabulary_occurrences[word]\n",
    "        tf=word_occur/tot_words_in_doc\n",
    "\n",
    "        #inverse data frequency= log(#tot_docs/#docs_containing_term_i)\n",
    "        tot_docs=len(dataframe)\n",
    "        docs_with_word=len(inverted_index[term_id])\n",
    "        idf=math.log(tot_docs/docs_with_word, 10)\n",
    "\n",
    "        #tf-idf\n",
    "        tf_idf=tf*idf        \n",
    "        \n",
    "        norm1 += np.power(tf_idf, 2)\n",
    "\n",
    "    return norm1\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e18befb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07765361421590745\n"
     ]
    }
   ],
   "source": [
    "dict_documents=documents_to_word(animes_df, 1, vocabulary) \n",
    "print(dict_documents)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8db1dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query):\n",
    "\n",
    "    query=pre_process_query(query)\n",
    "    cos_similarities = []\n",
    "    ti_query={}\n",
    "    # Aggiungere URL\n",
    "    q_match=pd.DataFrame(columns=['Title', 'Description', 'Similarity'])\n",
    "    query=query.split()\n",
    "\n",
    "    for word in query:\n",
    "        tf=query.count(word)/len(query)\n",
    "        term_id=vocabulary[word]\n",
    "        idf=np.log(len(second_inverted_index)/len(second_inverted_index[term_id]))\n",
    "        ti_query[word]=tf*idf\n",
    "\n",
    "\n",
    "    lst_doc = []\n",
    "    for term in query:\n",
    "        if term in vocabulary:\n",
    "            term_id = vocabulary[term]\n",
    "            lst_doc.append(inverted_index[term_id])\n",
    "        else:\n",
    "            print(\"no doc found\")\n",
    "\n",
    "    set_doc=lst_doc[0]\n",
    "    for x in lst_doc:\n",
    "        set_doc=set(set_doc).intersection(set(x))\n",
    "\n",
    "\n",
    "    set_doc=sorted(set_doc)\n",
    "    set_doc=list(set_doc)\n",
    "\n",
    "    #denominatore 2\n",
    "    norm2 = 0\n",
    "\n",
    "    for q in query:\n",
    "        i = ti_query[q]\n",
    "        norm2 += np.power(i, 2)\n",
    "    norm2 = np.sqrt(norm2)\n",
    "\n",
    "\n",
    "    for term in query:\n",
    "        term_id=vocabulary[term]\n",
    "       \n",
    "        \n",
    "        #prodotto scalare tra tf_idf * #numero occorrenze del termine nella query\n",
    "        for i, doc in enumerate(set_doc):\n",
    "            c = 0\n",
    "            tf_idf=second_inverted_index[term_id][i][1]\n",
    "            c +=  tf_idf * ti_query[term]\n",
    "\n",
    "            #denominatore 1\n",
    "            \n",
    "            norm1=documents_to_word(animes_df, doc, vocabulary) \n",
    "\n",
    "\n",
    "            norm1 = np.sqrt(norm1)\n",
    "\n",
    "\n",
    "            tot=c / (norm1 * norm2)\n",
    "            cos_similarities.append(tot)\n",
    "\n",
    "    \n",
    "\n",
    "    for i, doc in enumerate(set_doc):\n",
    "        # Aggiungere URL\n",
    "        q_match.at[i, ['Title', 'Description', 'Similarity']] = animes_original.iloc[doc]['Title'], animes_original.iloc[doc]['Description'], cos_similarities[i]\n",
    "        q_match.sort_values(by=['Similarity'], ascending=False, inplace=True)\n",
    "        q_match.reset_index(inplace=True, drop=True)\n",
    "    #display(q_match)\n",
    "    return q_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3366c68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K×Drop!!</td>\n",
       "      <td>['Endou, a night guard, was patrolling the mal...</td>\n",
       "      <td>0.396982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hotaru Koi</td>\n",
       "      <td>['Kids go looking for fireflies in the night.']</td>\n",
       "      <td>0.366678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yoru no Kuni</td>\n",
       "      <td>['Singer Aimer and Director Ryo-timo\\'s origin...</td>\n",
       "      <td>0.351126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fr/day Night</td>\n",
       "      <td>['A music video for the Phatmans After School ...</td>\n",
       "      <td>0.327005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maou Dante</td>\n",
       "      <td>['While sleeping one night, Ryo Utsugi had a n...</td>\n",
       "      <td>0.300868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Ginga Eiyuu Densetsu Gaiden</td>\n",
       "      <td>[&lt;i&gt;Ginga Eiyuu Densetsu Gaiden&lt;/i&gt;, ' is the ...</td>\n",
       "      <td>0.034592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Precure All Stars DX the Dance Live♥: Miracle ...</td>\n",
       "      <td>['Precure All-Stars DX the Dance Live: Miracle...</td>\n",
       "      <td>0.03438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Takane no Jitensha</td>\n",
       "      <td>['An 11-year-old boy, Takasumi Takane, will be...</td>\n",
       "      <td>0.033124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Shin Tennis no Ouji-sama Specials</td>\n",
       "      <td>[&lt;b&gt;Shouhei Zenya&lt;/b&gt;, ' (The Night Before The...</td>\n",
       "      <td>0.030882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Boku wa Imouto ni Koi wo Suru</td>\n",
       "      <td>[\"Yori and his twin sister Iku used to be very...</td>\n",
       "      <td>0.029556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0                                             K×Drop!!   \n",
       "1                                           Hotaru Koi   \n",
       "2                                         Yoru no Kuni   \n",
       "3                                         Fr/day Night   \n",
       "4                                           Maou Dante   \n",
       "..                                                 ...   \n",
       "495                        Ginga Eiyuu Densetsu Gaiden   \n",
       "496  Precure All Stars DX the Dance Live♥: Miracle ...   \n",
       "497                                 Takane no Jitensha   \n",
       "498                  Shin Tennis no Ouji-sama Specials   \n",
       "499                      Boku wa Imouto ni Koi wo Suru   \n",
       "\n",
       "                                           Description Similarity  \n",
       "0    ['Endou, a night guard, was patrolling the mal...   0.396982  \n",
       "1      ['Kids go looking for fireflies in the night.']   0.366678  \n",
       "2    ['Singer Aimer and Director Ryo-timo\\'s origin...   0.351126  \n",
       "3    ['A music video for the Phatmans After School ...   0.327005  \n",
       "4    ['While sleeping one night, Ryo Utsugi had a n...   0.300868  \n",
       "..                                                 ...        ...  \n",
       "495  [<i>Ginga Eiyuu Densetsu Gaiden</i>, ' is the ...   0.034592  \n",
       "496  ['Precure All-Stars DX the Dance Live: Miracle...    0.03438  \n",
       "497  ['An 11-year-old boy, Takasumi Takane, will be...   0.033124  \n",
       "498  [<b>Shouhei Zenya</b>, ' (The Night Before The...   0.030882  \n",
       "499  [\"Yori and his twin sister Iku used to be very...   0.029556  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity('night')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db38791",
   "metadata": {},
   "source": [
    "## 3. Define a new score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2322a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c7aa32b",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cabccc",
   "metadata": {},
   "source": [
    "### aggiungere pseudo codice\n",
    "In the given problem, the goal is to choose the maximum number of appointments such as the appointments choosen are not consecutive. It means that the personal trainer want needs a break between appointments and so he can't accept two consecutive request. Obviously, all the request are consider in chronological order. The input data is just a list of the requested appointment, and we want to maximize the sum of value (not adiacent) contained in the list.\n",
    "\n",
    "One solution could be the followed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7080a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_sum(arr):\n",
    "    a = 0\n",
    "    b = 0\n",
    "    lst=[]\n",
    "    for i in arr:\n",
    "\n",
    "        if b>a:\n",
    "            c= b\n",
    "        else:\n",
    "            c=a\n",
    "         \n",
    "        a = b + i\n",
    "        b = c\n",
    "     \n",
    "    # return max of a and b\n",
    "    return (b if b>a else a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appointments required: [30, 40, 25, 10, 20, 50, 30, 20] \n",
      "\n",
      "total duration (of the accepted appointment):  125\n"
     ]
    }
   ],
   "source": [
    "app = [30, 40, 25, 10, 20, 50, 30, 20] \n",
    "print('Appointments required:', app, '\\n') \n",
    "\n",
    "duration=find_max_sum(app)\n",
    "#print('List of appointment accepted: ', lst)\n",
    "print('total duration (of the accepted appointment): ', duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dcfc74",
   "metadata": {},
   "source": [
    "In this program we observe each element of the list twice (one from the even index, and one from the odd index), both of this going through the list with a step of 2. Thenwe add the odd and even values ​​respectively, and then compare them. At the end, we choose the path that maximize the sum."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
